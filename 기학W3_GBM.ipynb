{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxw902Dx7w6m",
        "outputId": "e663a979-06d7-427e-f4f3-10531c4eb7a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.7)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UTzViB9NxLCm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'monunmon.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "data['binary_label'] = data['Label'].apply(lambda x: 0 if x == -1 else 1)  # 이진 레이블 생성\n",
        "X = data.drop(['Label', 'binary_label'], axis=1)  # 특성 데이터\n",
        "y = data['binary_label']  # 이진 레이블\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "aI-C6eG35c7i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Classifier 사용\n",
        "gbc = GradientBoostingClassifier(random_state=42)\n",
        "gbc.fit(X_train, y_train)\n",
        "y_pred_gbc = gbc.predict(X_test)\n",
        "gbc_acc = accuracy_score(y_test, y_pred_gbc)\n",
        "gbc_f1 = f1_score(y_test, y_pred_gbc)\n",
        "\n",
        "print(\"Gradient Boosting Classifier\")\n",
        "print(f\"Accuracy: {gbc_acc:.4f}\")\n",
        "print(f\"F1-Score: {gbc_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yAANcl85i15",
        "outputId": "421f817a-7d95-4b31-a0a9-5b4b47e32674"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Classifier\n",
            "Accuracy: 0.8974\n",
            "F1-Score: 0.9436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Classifier\n",
        "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
        "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"XGBoost Classifier\")\n",
        "print(f\"Accuracy: {xgb_acc:.4f}\")\n",
        "print(f\"F1-Score: {xgb_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga3lXBxR6J_8",
        "outputId": "388eedf2-05e0-4964-b37e-2e13a7e1e2d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [05:33:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Classifier\n",
            "Accuracy: 0.9100\n",
            "F1-Score: 0.9497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM Classifier\n",
        "lgbm = LGBMClassifier(random_state=42)\n",
        "lgbm.fit(X_train, y_train)\n",
        "y_pred_lgbm = lgbm.predict(X_test)\n",
        "lgbm_acc = accuracy_score(y_test, y_pred_lgbm)\n",
        "lgbm_f1 = f1_score(y_test, y_pred_lgbm)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"LightGBM Classifier\")\n",
        "print(f\"Accuracy: {lgbm_acc:.4f}\")\n",
        "print(f\"F1-Score: {lgbm_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goVnVwBD7NCo",
        "outputId": "7058dbf6-a94f-4056-e447-21cc4389316d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 13282, number of negative: 2118\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004584 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3345\n",
            "[LightGBM] [Info] Number of data points in the train set: 15400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.862468 -> initscore=1.835937\n",
            "[LightGBM] [Info] Start training from score 1.835937\n",
            "LightGBM Classifier\n",
            "Accuracy: 0.9105\n",
            "F1-Score: 0.9504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5K0bti-Q9cEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CatBoost Classifier\n",
        "catboost = CatBoostClassifier(random_state=42, verbose=0)  # verbose=0으로 출력 제한\n",
        "catboost.fit(X_train, y_train)\n",
        "y_pred_catboost = catboost.predict(X_test)\n",
        "catboost_acc = accuracy_score(y_test, y_pred_catboost)\n",
        "catboost_f1 = f1_score(y_test, y_pred_catboost)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"CatBoost Classifier\")\n",
        "print(f\"Accuracy: {catboost_acc:.4f}\")\n",
        "print(f\"F1-Score: {catboost_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RJfku-z7fSj",
        "outputId": "7867fef5-47d3-46b3-aacb-4e463af34255"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost Classifier\n",
            "Accuracy: 0.9114\n",
            "F1-Score: 0.9508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaBoost Classifier 사용\n",
        "abc = AdaBoostClassifier(random_state=42)\n",
        "abc.fit(X_train, y_train)\n",
        "y_pred_abc = abc.predict(X_test)\n",
        "abc_acc = accuracy_score(y_test, y_pred_abc)\n",
        "abc_f1 = f1_score(y_test, y_pred_abc)\n",
        "\n",
        "print(\"\\nAdaBoost Classifier\")\n",
        "print(f\"Accuracy: {abc_acc:.4f}\")\n",
        "print(f\"F1-Score: {abc_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbc7tH7753sC",
        "outputId": "129b2fd2-aa4e-4247-99d7-2630123e10fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AdaBoost Classifier\n",
            "Accuracy: 0.8941\n",
            "F1-Score: 0.9417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wbcQDNetAjA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "성능 개선\n",
        "1. 데이터 전처리\n",
        "\n",
        "  a. 데이터 불균형: mon 과 unmon 19000:3000 으로 불균형\n",
        "\n",
        "        (1)  SMOTE 기법(소수 클래스 늘리기) 사용\n",
        "      \n",
        "  b. feature selection -> 보류\n",
        "\n",
        "  c. 새로운 특성 생성"
      ],
      "metadata": {
        "id": "kiZTZa4x9dIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDbLdc0N-frl",
        "outputId": "918fb5bd-1c48-4c6c-d028-868cd972d3d1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "data['binary_label'] = data['Label'].apply(lambda x: 0 if x == -1 else 1)  # 이진 레이블 생성\n",
        "X = data.drop(['Label', 'binary_label'], axis=1)  # 특성 데이터\n",
        "y = data['binary_label']  # 이진 레이블\n",
        "\n",
        "# SMOTE 적용\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "print(\"Original dataset size:\", X.shape[0])\n",
        "print(\"Resampled dataset size:\", X_resampled.shape[0])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s6Df9st-o3w",
        "outputId": "f815d2b1-7930-452d-8474-cab24e446cba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset size: 22000\n",
            "Resampled dataset size: 38000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Classifier 사용\n",
        "gbc = GradientBoostingClassifier(random_state=42)\n",
        "gbc.fit(X_train, y_train)\n",
        "y_pred_gbc = gbc.predict(X_test)\n",
        "gbc_acc = accuracy_score(y_test, y_pred_gbc)\n",
        "gbc_f1 = f1_score(y_test, y_pred_gbc)\n",
        "\n",
        "print(\"Gradient Boosting Classifier\")\n",
        "print(f\"Accuracy: {gbc_acc:.4f}\")\n",
        "print(f\"F1-Score: {gbc_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVxkdNV1-wNZ",
        "outputId": "db610301-9f0f-4510-80c9-0f90a6e6be0c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Classifier\n",
            "Accuracy: 0.8896\n",
            "F1-Score: 0.8957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaBoost Classifier 사용\n",
        "abc = AdaBoostClassifier(random_state=42)\n",
        "abc.fit(X_train, y_train)\n",
        "y_pred_abc = abc.predict(X_test)\n",
        "abc_acc = accuracy_score(y_test, y_pred_abc)\n",
        "abc_f1 = f1_score(y_test, y_pred_abc)\n",
        "\n",
        "print(\"\\nAdaBoost Classifier\")\n",
        "print(f\"Accuracy: {abc_acc:.4f}\")\n",
        "print(f\"F1-Score: {abc_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioYCqwtCAnDC",
        "outputId": "ad8297a5-01ed-46a5-f06b-607f75a67964"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AdaBoost Classifier\n",
            "Accuracy: 0.8678\n",
            "F1-Score: 0.8687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CatBoost Classifier\n",
        "catboost = CatBoostClassifier(random_state=42, verbose=0)  # verbose=0으로 출력 제한\n",
        "catboost.fit(X_train, y_train)\n",
        "y_pred_catboost = catboost.predict(X_test)\n",
        "catboost_acc = accuracy_score(y_test, y_pred_catboost)\n",
        "catboost_f1 = f1_score(y_test, y_pred_catboost)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"CatBoost Classifier\")\n",
        "print(f\"Accuracy: {catboost_acc:.4f}\")\n",
        "print(f\"F1-Score: {catboost_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMj9D0TEAwXA",
        "outputId": "4741a474-4fe4-4b41-ee5e-293fcf3f9967"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost Classifier\n",
            "Accuracy: 0.9267\n",
            "F1-Score: 0.9295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM Classifier\n",
        "lgbm = LGBMClassifier(random_state=42)\n",
        "lgbm.fit(X_train, y_train)\n",
        "y_pred_lgbm = lgbm.predict(X_test)\n",
        "lgbm_acc = accuracy_score(y_test, y_pred_lgbm)\n",
        "lgbm_f1 = f1_score(y_test, y_pred_lgbm)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"LightGBM Classifier\")\n",
        "print(f\"Accuracy: {lgbm_acc:.4f}\")\n",
        "print(f\"F1-Score: {lgbm_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH066S1rA3x6",
        "outputId": "600f24c7-eb9e-4277-c978-6cc2b0d8cc75"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 13310, number of negative: 13290\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012266 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 26600, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "LightGBM Classifier\n",
            "Accuracy: 0.9181\n",
            "F1-Score: 0.9217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Classifier\n",
        "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
        "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"XGBoost Classifier\")\n",
        "print(f\"Accuracy: {xgb_acc:.4f}\")\n",
        "print(f\"F1-Score: {xgb_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfZCmy0lA4AF",
        "outputId": "ea7a9795-cd9f-481b-d521-ad388948c502"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:02:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Classifier\n",
            "Accuracy: 0.9306\n",
            "F1-Score: 0.9327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GBC와 AdaBoost:**\n",
        "\n",
        "SMOTE로 생성된 합성 데이터가 원본 데이터와 다소 다른 분포를 가질 경우, 모델 성능이 저하될 가능성이 높음.\n",
        "특히 AdaBoost는 잘못된 데이터에 민감함.\n",
        "\n",
        "\n",
        "**CatBoost, LightGBM, XGBoost:**\n",
        "\n",
        "이러한 알고리즘들은 데이터 노이즈와 불균형에 더 잘 대처하는 구조를 가짐."
      ],
      "metadata": {
        "id": "bSf-uP3TBOx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#하이퍼파라미터 튜닝"
      ],
      "metadata": {
        "id": "cFlPCa_zCMaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "metadata": {
        "id": "lPmCnR-MCR6s"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adaboost\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
        "}\n",
        "\n",
        "ada = AdaBoostClassifier(random_state=42)\n",
        "grid_search_ada = GridSearchCV(estimator=ada, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2)\n",
        "grid_search_ada.fit(X_train, y_train)\n",
        "\n",
        "best_ada = grid_search_ada.best_estimator_\n",
        "y_pred_ada = best_ada.predict(X_test)\n",
        "ada_accuracy = accuracy_score(y_test, y_pred_ada)\n",
        "ada_f1 = f1_score(y_test, y_pred_ada)\n",
        "\n",
        "# 출력\n",
        "print(\"Adaboost Best Parameters:\", grid_search_ada.best_params_)\n",
        "print(f\"Adaboost Accuracy: {ada_accuracy:.4f}\")\n",
        "print(f\"Adaboost F1-Score: {ada_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhLABishB7HK",
        "outputId": "83bdb735-4691-4dc8-b402-ce5f81f8d241"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   6.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   5.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   7.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=  11.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=  10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=  10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=  22.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=  23.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=  25.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=  14.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=  13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   6.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   6.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   6.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.1, n_estimators=200; total time=  13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.1, n_estimators=200; total time=  13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.1, n_estimators=200; total time=  13.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.1, n_estimators=200; total time=  17.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.1, n_estimators=200; total time=  18.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=0.5, n_estimators=50; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=0.5, n_estimators=50; total time=   4.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=0.5, n_estimators=50; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=0.5, n_estimators=50; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=0.5, n_estimators=50; total time=   3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.5, n_estimators=100; total time=  11.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.5, n_estimators=100; total time=   7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.5, n_estimators=100; total time=   6.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.5, n_estimators=100; total time=   7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.5, n_estimators=100; total time=   6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.5, n_estimators=200; total time=  13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.5, n_estimators=200; total time=  13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.5, n_estimators=200; total time=  13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.5, n_estimators=200; total time=  13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=0.5, n_estimators=200; total time=  13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=1.0, n_estimators=50; total time=   3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=1.0, n_estimators=50; total time=   4.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=1.0, n_estimators=50; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=1.0, n_estimators=50; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END .................learning_rate=1.0, n_estimators=50; total time=   3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=1.0, n_estimators=100; total time=   7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=1.0, n_estimators=100; total time=   6.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=1.0, n_estimators=100; total time=   7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=1.0, n_estimators=100; total time=   6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=1.0, n_estimators=100; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=1.0, n_estimators=200; total time=  13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=1.0, n_estimators=200; total time=  13.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=1.0, n_estimators=200; total time=  13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=1.0, n_estimators=200; total time=  13.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................learning_rate=1.0, n_estimators=200; total time=  13.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adaboost Best Parameters: {'learning_rate': 1.0, 'n_estimators': 200}\n",
            "Adaboost Accuracy: 0.8897\n",
            "Adaboost F1-Score: 0.8956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LGBM\n",
        "\n",
        "param_dist = {\n",
        "    'num_leaves': [20, 31, 40, 50],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [-1, 10, 20],\n",
        "    'min_child_samples': [10, 20, 30],\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV 수행\n",
        "lgbm = LGBMClassifier(random_state=42)\n",
        "random_search_lgbm = RandomizedSearchCV(estimator=lgbm, param_distributions=param_dist, n_iter=20, cv=5, scoring='accuracy', verbose=2, random_state=42)\n",
        "random_search_lgbm.fit(X_train, y_train)\n",
        "\n",
        "# 최적 모델로 테스트 데이터 평가\n",
        "best_lgbm = random_search_lgbm.best_estimator_\n",
        "y_pred_lgbm = best_lgbm.predict(X_test)\n",
        "lgbm_accuracy = accuracy_score(y_test, y_pred_lgbm)\n",
        "lgbm_f1 = f1_score(y_test, y_pred_lgbm)\n",
        "\n",
        "print(\"LightGBM Best Parameters:\", random_search_lgbm.best_params_)\n",
        "print(f\"LightGBM Accuracy: {lgbm_accuracy:.4f}\")\n",
        "print(f\"LightGBM F1-Score: {lgbm_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oog3a7loC2Av",
        "outputId": "87b26c7c-9f12-4b6e-8f16-f46a2a59c201"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001869 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=20, min_child_samples=30, n_estimators=200, num_leaves=20; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003618 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=20, min_child_samples=30, n_estimators=200, num_leaves=20; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004029 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=20, min_child_samples=30, n_estimators=200, num_leaves=20; total time=   0.9s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005999 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=20, min_child_samples=30, n_estimators=200, num_leaves=20; total time=   1.1s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005474 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=20, min_child_samples=30, n_estimators=200, num_leaves=20; total time=   1.1s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005575 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=50; total time=   1.0s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001166 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=50; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003498 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=50; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003813 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=50; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003957 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=50; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003784 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=20; total time=   0.5s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003702 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=20; total time=   0.5s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003496 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=20; total time=   0.5s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003748 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=20; total time=   0.5s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003680 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=20; total time=   0.5s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003525 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=-1, min_child_samples=30, n_estimators=200, num_leaves=40; total time=   1.2s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003609 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=-1, min_child_samples=30, n_estimators=200, num_leaves=40; total time=   1.2s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003621 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=-1, min_child_samples=30, n_estimators=200, num_leaves=40; total time=   1.2s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003389 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=-1, min_child_samples=30, n_estimators=200, num_leaves=40; total time=   1.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005531 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=-1, min_child_samples=30, n_estimators=200, num_leaves=40; total time=   1.9s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005708 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=10, min_child_samples=30, n_estimators=500, num_leaves=20; total time=   1.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003680 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=10, min_child_samples=30, n_estimators=500, num_leaves=20; total time=   1.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003895 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=10, min_child_samples=30, n_estimators=500, num_leaves=20; total time=   1.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001108 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=10, min_child_samples=30, n_estimators=500, num_leaves=20; total time=   1.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003659 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=10, min_child_samples=30, n_estimators=500, num_leaves=20; total time=   1.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003706 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.1, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   2.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005701 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.1, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   3.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003782 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.1, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   2.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003423 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.1, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   2.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004540 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.1, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   2.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003766 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=20, min_child_samples=20, n_estimators=200, num_leaves=40; total time=   1.0s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004201 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=20, min_child_samples=20, n_estimators=200, num_leaves=40; total time=   1.3s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005691 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=20, min_child_samples=20, n_estimators=200, num_leaves=40; total time=   1.5s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005896 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=20, min_child_samples=20, n_estimators=200, num_leaves=40; total time=   1.4s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003458 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=20, min_child_samples=20, n_estimators=200, num_leaves=40; total time=   1.0s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003740 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=500, num_leaves=40; total time=   2.4s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003391 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=500, num_leaves=40; total time=   2.4s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003822 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=500, num_leaves=40; total time=   2.4s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003604 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=500, num_leaves=40; total time=   2.9s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005869 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=500, num_leaves=40; total time=   3.2s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003461 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=100, num_leaves=50; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003493 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=100, num_leaves=50; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003720 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=100, num_leaves=50; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003689 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=100, num_leaves=50; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003635 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=100, num_leaves=50; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003609 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=200, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003412 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=200, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003463 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=200, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003753 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=200, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003624 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=200, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003602 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=500, num_leaves=31; total time=   2.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005856 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=500, num_leaves=31; total time=   2.9s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003841 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=500, num_leaves=31; total time=   2.1s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003468 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=500, num_leaves=31; total time=   2.1s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003749 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=10, n_estimators=500, num_leaves=31; total time=   2.1s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005207 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=10, min_child_samples=30, n_estimators=500, num_leaves=40; total time=   2.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003332 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=10, min_child_samples=30, n_estimators=500, num_leaves=40; total time=   4.1s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003668 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=10, min_child_samples=30, n_estimators=500, num_leaves=40; total time=   2.9s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003692 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=10, min_child_samples=30, n_estimators=500, num_leaves=40; total time=   2.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003562 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=10, min_child_samples=30, n_estimators=500, num_leaves=40; total time=   2.9s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003448 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=200, num_leaves=50; total time=   1.3s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005900 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=200, num_leaves=50; total time=   1.9s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005330 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=200, num_leaves=50; total time=   1.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003627 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=200, num_leaves=50; total time=   1.3s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003662 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=20, min_child_samples=20, n_estimators=200, num_leaves=50; total time=   1.3s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003808 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=-1, min_child_samples=10, n_estimators=500, num_leaves=40; total time=   2.3s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003617 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=-1, min_child_samples=10, n_estimators=500, num_leaves=40; total time=   2.4s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003380 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=-1, min_child_samples=10, n_estimators=500, num_leaves=40; total time=   2.4s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008041 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=-1, min_child_samples=10, n_estimators=500, num_leaves=40; total time=   3.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003379 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=-1, min_child_samples=10, n_estimators=500, num_leaves=40; total time=   2.4s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003457 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=10, min_child_samples=10, n_estimators=100, num_leaves=50; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003493 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=10, min_child_samples=10, n_estimators=100, num_leaves=50; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003787 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=10, min_child_samples=10, n_estimators=100, num_leaves=50; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003553 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=10, min_child_samples=10, n_estimators=100, num_leaves=50; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003763 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=10, min_child_samples=10, n_estimators=100, num_leaves=50; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003810 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=30, n_estimators=200, num_leaves=31; total time=   1.0s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003600 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=30, n_estimators=200, num_leaves=31; total time=   1.0s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003844 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=30, n_estimators=200, num_leaves=31; total time=   1.0s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003794 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=30, n_estimators=200, num_leaves=31; total time=   1.0s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006256 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=30, n_estimators=200, num_leaves=31; total time=   1.5s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007926 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.2, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=40; total time=   3.1s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003383 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.2, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=40; total time=   2.3s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003553 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.2, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=40; total time=   2.3s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003483 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.2, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=40; total time=   2.3s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003527 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[CV] END learning_rate=0.2, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=40; total time=   2.3s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005798 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005677 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006145 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005868 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006109 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=20; total time=   0.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004884 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=-1, min_child_samples=10, n_estimators=500, num_leaves=31; total time=   2.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003611 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=-1, min_child_samples=10, n_estimators=500, num_leaves=31; total time=   2.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003577 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=-1, min_child_samples=10, n_estimators=500, num_leaves=31; total time=   2.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003462 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=-1, min_child_samples=10, n_estimators=500, num_leaves=31; total time=   2.9s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005779 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.01, max_depth=-1, min_child_samples=10, n_estimators=500, num_leaves=31; total time=   3.6s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003498 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.1, max_depth=10, min_child_samples=30, n_estimators=200, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003507 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.1, max_depth=10, min_child_samples=30, n_estimators=200, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003507 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.1, max_depth=10, min_child_samples=30, n_estimators=200, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003801 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.1, max_depth=10, min_child_samples=30, n_estimators=200, num_leaves=20; total time=   0.7s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003674 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[CV] END learning_rate=0.1, max_depth=10, min_child_samples=30, n_estimators=200, num_leaves=20; total time=   0.8s\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 13310, number of negative: 13290\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004856 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 26600, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "LightGBM Best Parameters: {'num_leaves': 40, 'n_estimators': 500, 'min_child_samples': 10, 'max_depth': -1, 'learning_rate': 0.2}\n",
            "LightGBM Accuracy: 0.9432\n",
            "LightGBM F1-Score: 0.9445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Catboost\n",
        "param_grid = {\n",
        "    'iterations': [100, 200, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'depth': [4, 6, 8, 10],\n",
        "    'l2_leaf_reg': [1, 3, 5, 7]\n",
        "}\n",
        "\n",
        "catboost = CatBoostClassifier(random_state=42, verbose=0)\n",
        "grid_search_result = catboost.grid_search(param_grid, X_train, y_train, cv=5, stratified=True)\n",
        "\n",
        "# 최적 모델로 테스트 데이터 평가\n",
        "best_catboost = CatBoostClassifier(**grid_search_result['params'], random_state=42, verbose=0)\n",
        "best_catboost.fit(X_train, y_train)\n",
        "y_pred_catboost = best_catboost.predict(X_test)\n",
        "catboost_accuracy = accuracy_score(y_test, y_pred_catboost)\n",
        "catboost_f1 = f1_score(y_test, y_pred_catboost)\n",
        "\n",
        "print(\"CatBoost Best Parameters:\", grid_search_result['params'])\n",
        "print(f\"CatBoost Accuracy: {catboost_accuracy:.4f}\")\n",
        "print(f\"CatBoost F1-Score: {catboost_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8udXoLJDAsu",
        "outputId": "d433c039-d6ba-4430-a6c4-6b11a850a1fd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bestTest = 0.4574837366\n",
            "bestIteration = 99\n",
            "\n",
            "0:\tloss: 0.4574837\tbest: 0.4574837 (0)\ttotal: 903ms\tremaining: 2m 9s\n",
            "\n",
            "bestTest = 0.2934673244\n",
            "bestIteration = 99\n",
            "\n",
            "1:\tloss: 0.2934673\tbest: 0.2934673 (1)\ttotal: 1.72s\tremaining: 2m 2s\n",
            "\n",
            "bestTest = 0.2590088178\n",
            "bestIteration = 99\n",
            "\n",
            "2:\tloss: 0.2590088\tbest: 0.2590088 (2)\ttotal: 2.53s\tremaining: 1m 58s\n",
            "\n",
            "bestTest = 0.4620257781\n",
            "bestIteration = 99\n",
            "\n",
            "3:\tloss: 0.4620258\tbest: 0.2590088 (2)\ttotal: 4.11s\tremaining: 2m 23s\n",
            "\n",
            "bestTest = 0.3015910996\n",
            "bestIteration = 99\n",
            "\n",
            "4:\tloss: 0.3015911\tbest: 0.2590088 (2)\ttotal: 5.74s\tremaining: 2m 39s\n",
            "\n",
            "bestTest = 0.2638180221\n",
            "bestIteration = 99\n",
            "\n",
            "5:\tloss: 0.2638180\tbest: 0.2590088 (2)\ttotal: 6.8s\tremaining: 2m 36s\n",
            "\n",
            "bestTest = 0.4662033519\n",
            "bestIteration = 99\n",
            "\n",
            "6:\tloss: 0.4662034\tbest: 0.2590088 (2)\ttotal: 7.6s\tremaining: 2m 28s\n",
            "\n",
            "bestTest = 0.3018752537\n",
            "bestIteration = 99\n",
            "\n",
            "7:\tloss: 0.3018753\tbest: 0.2590088 (2)\ttotal: 8.43s\tremaining: 2m 23s\n",
            "\n",
            "bestTest = 0.2665907447\n",
            "bestIteration = 99\n",
            "\n",
            "8:\tloss: 0.2665907\tbest: 0.2590088 (2)\ttotal: 9.27s\tremaining: 2m 18s\n",
            "\n",
            "bestTest = 0.4710396706\n",
            "bestIteration = 99\n",
            "\n",
            "9:\tloss: 0.4710397\tbest: 0.2590088 (2)\ttotal: 10.1s\tremaining: 2m 15s\n",
            "\n",
            "bestTest = 0.3041944321\n",
            "bestIteration = 99\n",
            "\n",
            "10:\tloss: 0.3041944\tbest: 0.2590088 (2)\ttotal: 10.9s\tremaining: 2m 12s\n",
            "\n",
            "bestTest = 0.261699763\n",
            "bestIteration = 99\n",
            "\n",
            "11:\tloss: 0.2616998\tbest: 0.2590088 (2)\ttotal: 11.8s\tremaining: 2m 9s\n",
            "\n",
            "bestTest = 0.374065166\n",
            "bestIteration = 199\n",
            "\n",
            "12:\tloss: 0.3740652\tbest: 0.2590088 (2)\ttotal: 13.4s\tremaining: 2m 15s\n",
            "\n",
            "bestTest = 0.2621591379\n",
            "bestIteration = 199\n",
            "\n",
            "13:\tloss: 0.2621591\tbest: 0.2590088 (2)\ttotal: 15s\tremaining: 2m 19s\n",
            "\n",
            "bestTest = 0.2282399571\n",
            "bestIteration = 199\n",
            "\n",
            "14:\tloss: 0.2282400\tbest: 0.2282400 (14)\ttotal: 17.2s\tremaining: 2m 27s\n",
            "\n",
            "bestTest = 0.3778308491\n",
            "bestIteration = 199\n",
            "\n",
            "15:\tloss: 0.3778308\tbest: 0.2282400 (14)\ttotal: 20s\tremaining: 2m 40s\n",
            "\n",
            "bestTest = 0.2655607001\n",
            "bestIteration = 199\n",
            "\n",
            "16:\tloss: 0.2655607\tbest: 0.2282400 (14)\ttotal: 21.7s\tremaining: 2m 42s\n",
            "\n",
            "bestTest = 0.2331193284\n",
            "bestIteration = 199\n",
            "\n",
            "17:\tloss: 0.2331193\tbest: 0.2282400 (14)\ttotal: 23.4s\tremaining: 2m 43s\n",
            "\n",
            "bestTest = 0.3820725609\n",
            "bestIteration = 199\n",
            "\n",
            "18:\tloss: 0.3820726\tbest: 0.2282400 (14)\ttotal: 25.1s\tremaining: 2m 44s\n",
            "\n",
            "bestTest = 0.2657899507\n",
            "bestIteration = 199\n",
            "\n",
            "19:\tloss: 0.2657900\tbest: 0.2282400 (14)\ttotal: 26.8s\tremaining: 2m 45s\n",
            "\n",
            "bestTest = 0.2330620165\n",
            "bestIteration = 199\n",
            "\n",
            "20:\tloss: 0.2330620\tbest: 0.2282400 (14)\ttotal: 28.4s\tremaining: 2m 46s\n",
            "\n",
            "bestTest = 0.387751442\n",
            "bestIteration = 199\n",
            "\n",
            "21:\tloss: 0.3877514\tbest: 0.2282400 (14)\ttotal: 30.5s\tremaining: 2m 49s\n",
            "\n",
            "bestTest = 0.2660207779\n",
            "bestIteration = 199\n",
            "\n",
            "22:\tloss: 0.2660208\tbest: 0.2282400 (14)\ttotal: 33.4s\tremaining: 2m 55s\n",
            "\n",
            "bestTest = 0.2337257796\n",
            "bestIteration = 199\n",
            "\n",
            "23:\tloss: 0.2337258\tbest: 0.2282400 (14)\ttotal: 35.1s\tremaining: 2m 55s\n",
            "\n",
            "bestTest = 0.301481872\n",
            "bestIteration = 499\n",
            "\n",
            "24:\tloss: 0.3014819\tbest: 0.2282400 (14)\ttotal: 39.1s\tremaining: 3m 6s\n",
            "\n",
            "bestTest = 0.222318618\n",
            "bestIteration = 499\n",
            "\n",
            "25:\tloss: 0.2223186\tbest: 0.2223186 (25)\ttotal: 43.4s\tremaining: 3m 17s\n",
            "\n",
            "bestTest = 0.1942471751\n",
            "bestIteration = 498\n",
            "\n",
            "26:\tloss: 0.1942472\tbest: 0.1942472 (26)\ttotal: 49.2s\tremaining: 3m 33s\n",
            "\n",
            "bestTest = 0.301276304\n",
            "bestIteration = 499\n",
            "\n",
            "27:\tloss: 0.3012763\tbest: 0.1942472 (26)\ttotal: 53.4s\tremaining: 3m 41s\n",
            "\n",
            "bestTest = 0.2243787213\n",
            "bestIteration = 499\n",
            "\n",
            "28:\tloss: 0.2243787\tbest: 0.1942472 (26)\ttotal: 58.1s\tremaining: 3m 50s\n",
            "\n",
            "bestTest = 0.1979874926\n",
            "bestIteration = 499\n",
            "\n",
            "29:\tloss: 0.1979875\tbest: 0.1942472 (26)\ttotal: 1m 3s\tremaining: 4m\n",
            "\n",
            "bestTest = 0.3050358904\n",
            "bestIteration = 499\n",
            "\n",
            "30:\tloss: 0.3050359\tbest: 0.1942472 (26)\ttotal: 1m 7s\tremaining: 4m 6s\n",
            "\n",
            "bestTest = 0.2248975936\n",
            "bestIteration = 499\n",
            "\n",
            "31:\tloss: 0.2248976\tbest: 0.1942472 (26)\ttotal: 1m 12s\tremaining: 4m 14s\n",
            "\n",
            "bestTest = 0.1985769052\n",
            "bestIteration = 499\n",
            "\n",
            "32:\tloss: 0.1985769\tbest: 0.1942472 (26)\ttotal: 1m 17s\tremaining: 4m 21s\n",
            "\n",
            "bestTest = 0.3061193312\n",
            "bestIteration = 499\n",
            "\n",
            "33:\tloss: 0.3061193\tbest: 0.1942472 (26)\ttotal: 1m 21s\tremaining: 4m 24s\n",
            "\n",
            "bestTest = 0.2251812457\n",
            "bestIteration = 499\n",
            "\n",
            "34:\tloss: 0.2251812\tbest: 0.1942472 (26)\ttotal: 1m 27s\tremaining: 4m 33s\n",
            "\n",
            "bestTest = 0.2010443024\n",
            "bestIteration = 499\n",
            "\n",
            "35:\tloss: 0.2010443\tbest: 0.1942472 (26)\ttotal: 1m 32s\tremaining: 4m 36s\n",
            "\n",
            "bestTest = 0.4053965259\n",
            "bestIteration = 99\n",
            "\n",
            "36:\tloss: 0.4053965\tbest: 0.1942472 (26)\ttotal: 1m 33s\tremaining: 4m 29s\n",
            "\n",
            "bestTest = 0.2724671309\n",
            "bestIteration = 99\n",
            "\n",
            "37:\tloss: 0.2724671\tbest: 0.1942472 (26)\ttotal: 1m 34s\tremaining: 4m 23s\n",
            "\n",
            "bestTest = 0.2411505962\n",
            "bestIteration = 99\n",
            "\n",
            "38:\tloss: 0.2411506\tbest: 0.1942472 (26)\ttotal: 1m 35s\tremaining: 4m 18s\n",
            "\n",
            "bestTest = 0.4072074637\n",
            "bestIteration = 99\n",
            "\n",
            "39:\tloss: 0.4072075\tbest: 0.1942472 (26)\ttotal: 1m 37s\tremaining: 4m 12s\n",
            "\n",
            "bestTest = 0.2787604509\n",
            "bestIteration = 99\n",
            "\n",
            "40:\tloss: 0.2787605\tbest: 0.1942472 (26)\ttotal: 1m 39s\tremaining: 4m 9s\n",
            "\n",
            "bestTest = 0.2433832482\n",
            "bestIteration = 99\n",
            "\n",
            "41:\tloss: 0.2433832\tbest: 0.1942472 (26)\ttotal: 1m 41s\tremaining: 4m 6s\n",
            "\n",
            "bestTest = 0.4128045564\n",
            "bestIteration = 99\n",
            "\n",
            "42:\tloss: 0.4128046\tbest: 0.1942472 (26)\ttotal: 1m 42s\tremaining: 4m 1s\n",
            "\n",
            "bestTest = 0.2783573629\n",
            "bestIteration = 99\n",
            "\n",
            "43:\tloss: 0.2783574\tbest: 0.1942472 (26)\ttotal: 1m 44s\tremaining: 3m 56s\n",
            "\n",
            "bestTest = 0.2456748662\n",
            "bestIteration = 99\n",
            "\n",
            "44:\tloss: 0.2456749\tbest: 0.1942472 (26)\ttotal: 1m 45s\tremaining: 3m 52s\n",
            "\n",
            "bestTest = 0.4110089034\n",
            "bestIteration = 99\n",
            "\n",
            "45:\tloss: 0.4110089\tbest: 0.1942472 (26)\ttotal: 1m 46s\tremaining: 3m 47s\n",
            "\n",
            "bestTest = 0.2782629004\n",
            "bestIteration = 99\n",
            "\n",
            "46:\tloss: 0.2782629\tbest: 0.1942472 (26)\ttotal: 1m 48s\tremaining: 3m 43s\n",
            "\n",
            "bestTest = 0.2473430302\n",
            "bestIteration = 99\n",
            "\n",
            "47:\tloss: 0.2473430\tbest: 0.1942472 (26)\ttotal: 1m 49s\tremaining: 3m 38s\n",
            "\n",
            "bestTest = 0.3309230651\n",
            "bestIteration = 199\n",
            "\n",
            "48:\tloss: 0.3309231\tbest: 0.1942472 (26)\ttotal: 1m 53s\tremaining: 3m 39s\n",
            "\n",
            "bestTest = 0.2406915183\n",
            "bestIteration = 199\n",
            "\n",
            "49:\tloss: 0.2406915\tbest: 0.1942472 (26)\ttotal: 1m 56s\tremaining: 3m 38s\n",
            "\n",
            "bestTest = 0.2077968472\n",
            "bestIteration = 199\n",
            "\n",
            "50:\tloss: 0.2077968\tbest: 0.1942472 (26)\ttotal: 1m 58s\tremaining: 3m 36s\n",
            "\n",
            "bestTest = 0.338404991\n",
            "bestIteration = 199\n",
            "\n",
            "51:\tloss: 0.3384050\tbest: 0.1942472 (26)\ttotal: 2m 1s\tremaining: 3m 34s\n",
            "\n",
            "bestTest = 0.243509492\n",
            "bestIteration = 199\n",
            "\n",
            "52:\tloss: 0.2435095\tbest: 0.1942472 (26)\ttotal: 2m 3s\tremaining: 3m 32s\n",
            "\n",
            "bestTest = 0.209803046\n",
            "bestIteration = 199\n",
            "\n",
            "53:\tloss: 0.2098030\tbest: 0.1942472 (26)\ttotal: 2m 7s\tremaining: 3m 32s\n",
            "\n",
            "bestTest = 0.3395216795\n",
            "bestIteration = 199\n",
            "\n",
            "54:\tloss: 0.3395217\tbest: 0.1942472 (26)\ttotal: 2m 10s\tremaining: 3m 30s\n",
            "\n",
            "bestTest = 0.2470265715\n",
            "bestIteration = 199\n",
            "\n",
            "55:\tloss: 0.2470266\tbest: 0.1942472 (26)\ttotal: 2m 12s\tremaining: 3m 28s\n",
            "\n",
            "bestTest = 0.21298517\n",
            "bestIteration = 199\n",
            "\n",
            "56:\tloss: 0.2129852\tbest: 0.1942472 (26)\ttotal: 2m 15s\tremaining: 3m 26s\n",
            "\n",
            "bestTest = 0.3390369054\n",
            "bestIteration = 199\n",
            "\n",
            "57:\tloss: 0.3390369\tbest: 0.1942472 (26)\ttotal: 2m 17s\tremaining: 3m 23s\n",
            "\n",
            "bestTest = 0.2458231692\n",
            "bestIteration = 199\n",
            "\n",
            "58:\tloss: 0.2458232\tbest: 0.1942472 (26)\ttotal: 2m 21s\tremaining: 3m 23s\n",
            "\n",
            "bestTest = 0.2130226758\n",
            "bestIteration = 199\n",
            "\n",
            "59:\tloss: 0.2130227\tbest: 0.1942472 (26)\ttotal: 2m 24s\tremaining: 3m 21s\n",
            "\n",
            "bestTest = 0.2737141746\n",
            "bestIteration = 499\n",
            "\n",
            "60:\tloss: 0.2737142\tbest: 0.1942472 (26)\ttotal: 2m 30s\tremaining: 3m 24s\n",
            "\n",
            "bestTest = 0.197421757\n",
            "bestIteration = 499\n",
            "\n",
            "61:\tloss: 0.1974218\tbest: 0.1942472 (26)\ttotal: 2m 37s\tremaining: 3m 28s\n",
            "\n",
            "bestTest = 0.170817572\n",
            "bestIteration = 499\n",
            "\n",
            "62:\tloss: 0.1708176\tbest: 0.1708176 (62)\ttotal: 2m 43s\tremaining: 3m 30s\n",
            "\n",
            "bestTest = 0.2782082751\n",
            "bestIteration = 499\n",
            "\n",
            "63:\tloss: 0.2782083\tbest: 0.1708176 (62)\ttotal: 2m 52s\tremaining: 3m 35s\n",
            "\n",
            "bestTest = 0.2011734336\n",
            "bestIteration = 499\n",
            "\n",
            "64:\tloss: 0.2011734\tbest: 0.1708176 (62)\ttotal: 2m 58s\tremaining: 3m 36s\n",
            "\n",
            "bestTest = 0.1747642715\n",
            "bestIteration = 498\n",
            "\n",
            "65:\tloss: 0.1747643\tbest: 0.1708176 (62)\ttotal: 3m 6s\tremaining: 3m 40s\n",
            "\n",
            "bestTest = 0.2781311835\n",
            "bestIteration = 499\n",
            "\n",
            "66:\tloss: 0.2781312\tbest: 0.1708176 (62)\ttotal: 3m 12s\tremaining: 3m 41s\n",
            "\n",
            "bestTest = 0.2055178268\n",
            "bestIteration = 499\n",
            "\n",
            "67:\tloss: 0.2055178\tbest: 0.1708176 (62)\ttotal: 3m 20s\tremaining: 3m 44s\n",
            "\n",
            "bestTest = 0.1772327108\n",
            "bestIteration = 499\n",
            "\n",
            "68:\tloss: 0.1772327\tbest: 0.1708176 (62)\ttotal: 3m 26s\tremaining: 3m 44s\n",
            "\n",
            "bestTest = 0.2801826778\n",
            "bestIteration = 499\n",
            "\n",
            "69:\tloss: 0.2801827\tbest: 0.1708176 (62)\ttotal: 3m 34s\tremaining: 3m 46s\n",
            "\n",
            "bestTest = 0.2048722262\n",
            "bestIteration = 499\n",
            "\n",
            "70:\tloss: 0.2048722\tbest: 0.1708176 (62)\ttotal: 3m 40s\tremaining: 3m 46s\n",
            "\n",
            "bestTest = 0.1770063309\n",
            "bestIteration = 499\n",
            "\n",
            "71:\tloss: 0.1770063\tbest: 0.1708176 (62)\ttotal: 3m 48s\tremaining: 3m 48s\n",
            "\n",
            "bestTest = 0.3721750522\n",
            "bestIteration = 99\n",
            "\n",
            "72:\tloss: 0.3721751\tbest: 0.1708176 (62)\ttotal: 3m 50s\tremaining: 3m 44s\n",
            "\n",
            "bestTest = 0.2574323183\n",
            "bestIteration = 99\n",
            "\n",
            "73:\tloss: 0.2574323\tbest: 0.1708176 (62)\ttotal: 3m 53s\tremaining: 3m 40s\n",
            "\n",
            "bestTest = 0.2219577956\n",
            "bestIteration = 99\n",
            "\n",
            "74:\tloss: 0.2219578\tbest: 0.1708176 (62)\ttotal: 3m 56s\tremaining: 3m 37s\n",
            "\n",
            "bestTest = 0.3785009352\n",
            "bestIteration = 99\n",
            "\n",
            "75:\tloss: 0.3785009\tbest: 0.1708176 (62)\ttotal: 3m 59s\tremaining: 3m 34s\n",
            "\n",
            "bestTest = 0.2584420906\n",
            "bestIteration = 99\n",
            "\n",
            "76:\tloss: 0.2584421\tbest: 0.1708176 (62)\ttotal: 4m 1s\tremaining: 3m 30s\n",
            "\n",
            "bestTest = 0.2284102004\n",
            "bestIteration = 99\n",
            "\n",
            "77:\tloss: 0.2284102\tbest: 0.1708176 (62)\ttotal: 4m 4s\tremaining: 3m 26s\n",
            "\n",
            "bestTest = 0.380280933\n",
            "bestIteration = 99\n",
            "\n",
            "78:\tloss: 0.3802809\tbest: 0.1708176 (62)\ttotal: 4m 6s\tremaining: 3m 23s\n",
            "\n",
            "bestTest = 0.263108088\n",
            "bestIteration = 99\n",
            "\n",
            "79:\tloss: 0.2631081\tbest: 0.1708176 (62)\ttotal: 4m 10s\tremaining: 3m 20s\n",
            "\n",
            "bestTest = 0.2308812998\n",
            "bestIteration = 99\n",
            "\n",
            "80:\tloss: 0.2308813\tbest: 0.1708176 (62)\ttotal: 4m 13s\tremaining: 3m 16s\n",
            "\n",
            "bestTest = 0.3841059958\n",
            "bestIteration = 99\n",
            "\n",
            "81:\tloss: 0.3841060\tbest: 0.1708176 (62)\ttotal: 4m 15s\tremaining: 3m 13s\n",
            "\n",
            "bestTest = 0.2634448438\n",
            "bestIteration = 99\n",
            "\n",
            "82:\tloss: 0.2634448\tbest: 0.1708176 (62)\ttotal: 4m 18s\tremaining: 3m 9s\n",
            "\n",
            "bestTest = 0.230797408\n",
            "bestIteration = 99\n",
            "\n",
            "83:\tloss: 0.2307974\tbest: 0.1708176 (62)\ttotal: 4m 20s\tremaining: 3m 6s\n",
            "\n",
            "bestTest = 0.3103522072\n",
            "bestIteration = 199\n",
            "\n",
            "84:\tloss: 0.3103522\tbest: 0.1708176 (62)\ttotal: 4m 26s\tremaining: 3m 5s\n",
            "\n",
            "bestTest = 0.2238873728\n",
            "bestIteration = 199\n",
            "\n",
            "85:\tloss: 0.2238874\tbest: 0.1708176 (62)\ttotal: 4m 31s\tremaining: 3m 3s\n",
            "\n",
            "bestTest = 0.1864880518\n",
            "bestIteration = 199\n",
            "\n",
            "86:\tloss: 0.1864881\tbest: 0.1708176 (62)\ttotal: 4m 37s\tremaining: 3m 1s\n",
            "\n",
            "bestTest = 0.3165929697\n",
            "bestIteration = 199\n",
            "\n",
            "87:\tloss: 0.3165930\tbest: 0.1708176 (62)\ttotal: 4m 42s\tremaining: 2m 59s\n",
            "\n",
            "bestTest = 0.226914059\n",
            "bestIteration = 199\n",
            "\n",
            "88:\tloss: 0.2269141\tbest: 0.1708176 (62)\ttotal: 4m 47s\tremaining: 2m 57s\n",
            "\n",
            "bestTest = 0.1922905787\n",
            "bestIteration = 199\n",
            "\n",
            "89:\tloss: 0.1922906\tbest: 0.1708176 (62)\ttotal: 4m 53s\tremaining: 2m 56s\n",
            "\n",
            "bestTest = 0.3182459453\n",
            "bestIteration = 199\n",
            "\n",
            "90:\tloss: 0.3182459\tbest: 0.1708176 (62)\ttotal: 4m 58s\tremaining: 2m 54s\n",
            "\n",
            "bestTest = 0.2307128664\n",
            "bestIteration = 199\n",
            "\n",
            "91:\tloss: 0.2307129\tbest: 0.1708176 (62)\ttotal: 5m 4s\tremaining: 2m 52s\n",
            "\n",
            "bestTest = 0.1953506047\n",
            "bestIteration = 199\n",
            "\n",
            "92:\tloss: 0.1953506\tbest: 0.1708176 (62)\ttotal: 5m 10s\tremaining: 2m 50s\n",
            "\n",
            "bestTest = 0.3214094492\n",
            "bestIteration = 199\n",
            "\n",
            "93:\tloss: 0.3214094\tbest: 0.1708176 (62)\ttotal: 5m 14s\tremaining: 2m 47s\n",
            "\n",
            "bestTest = 0.2330320069\n",
            "bestIteration = 199\n",
            "\n",
            "94:\tloss: 0.2330320\tbest: 0.1708176 (62)\ttotal: 5m 21s\tremaining: 2m 45s\n",
            "\n",
            "bestTest = 0.197703324\n",
            "bestIteration = 199\n",
            "\n",
            "95:\tloss: 0.1977033\tbest: 0.1708176 (62)\ttotal: 5m 26s\tremaining: 2m 43s\n",
            "\n",
            "bestTest = 0.2578823274\n",
            "bestIteration = 499\n",
            "\n",
            "96:\tloss: 0.2578823\tbest: 0.1708176 (62)\ttotal: 5m 39s\tremaining: 2m 44s\n",
            "\n",
            "bestTest = 0.1770143088\n",
            "bestIteration = 499\n",
            "\n",
            "97:\tloss: 0.1770143\tbest: 0.1708176 (62)\ttotal: 6m 3s\tremaining: 2m 50s\n",
            "\n",
            "bestTest = 0.1510047818\n",
            "bestIteration = 498\n",
            "\n",
            "98:\tloss: 0.1510048\tbest: 0.1510048 (98)\ttotal: 6m 30s\tremaining: 2m 57s\n",
            "\n",
            "bestTest = 0.2610818153\n",
            "bestIteration = 499\n",
            "\n",
            "99:\tloss: 0.2610818\tbest: 0.1510048 (98)\ttotal: 6m 54s\tremaining: 3m 2s\n",
            "\n",
            "bestTest = 0.1821980786\n",
            "bestIteration = 499\n",
            "\n",
            "100:\tloss: 0.1821981\tbest: 0.1510048 (98)\ttotal: 7m 13s\tremaining: 3m 4s\n",
            "\n",
            "bestTest = 0.1601848318\n",
            "bestIteration = 499\n",
            "\n",
            "101:\tloss: 0.1601848\tbest: 0.1510048 (98)\ttotal: 7m 30s\tremaining: 3m 5s\n",
            "\n",
            "bestTest = 0.2629545836\n",
            "bestIteration = 499\n",
            "\n",
            "102:\tloss: 0.2629546\tbest: 0.1510048 (98)\ttotal: 7m 44s\tremaining: 3m 4s\n",
            "\n",
            "bestTest = 0.1866893123\n",
            "bestIteration = 499\n",
            "\n",
            "103:\tloss: 0.1866893\tbest: 0.1510048 (98)\ttotal: 7m 57s\tremaining: 3m 3s\n",
            "\n",
            "bestTest = 0.1614985252\n",
            "bestIteration = 499\n",
            "\n",
            "104:\tloss: 0.1614985\tbest: 0.1510048 (98)\ttotal: 8m 13s\tremaining: 3m 3s\n",
            "\n",
            "bestTest = 0.2645637637\n",
            "bestIteration = 499\n",
            "\n",
            "105:\tloss: 0.2645638\tbest: 0.1510048 (98)\ttotal: 8m 29s\tremaining: 3m 2s\n",
            "\n",
            "bestTest = 0.189553359\n",
            "bestIteration = 499\n",
            "\n",
            "106:\tloss: 0.1895534\tbest: 0.1510048 (98)\ttotal: 8m 42s\tremaining: 3m\n",
            "\n",
            "bestTest = 0.1633343091\n",
            "bestIteration = 499\n",
            "\n",
            "107:\tloss: 0.1633343\tbest: 0.1510048 (98)\ttotal: 8m 56s\tremaining: 2m 58s\n",
            "\n",
            "bestTest = 0.3570552925\n",
            "bestIteration = 99\n",
            "\n",
            "108:\tloss: 0.3570553\tbest: 0.1510048 (98)\ttotal: 9m 4s\tremaining: 2m 54s\n",
            "\n",
            "bestTest = 0.2419696999\n",
            "bestIteration = 99\n",
            "\n",
            "109:\tloss: 0.2419697\tbest: 0.1510048 (98)\ttotal: 9m 10s\tremaining: 2m 50s\n",
            "\n",
            "bestTest = 0.2071621841\n",
            "bestIteration = 99\n",
            "\n",
            "110:\tloss: 0.2071622\tbest: 0.1510048 (98)\ttotal: 9m 19s\tremaining: 2m 46s\n",
            "\n",
            "bestTest = 0.3636985032\n",
            "bestIteration = 99\n",
            "\n",
            "111:\tloss: 0.3636985\tbest: 0.1510048 (98)\ttotal: 9m 26s\tremaining: 2m 41s\n",
            "\n",
            "bestTest = 0.2487081979\n",
            "bestIteration = 99\n",
            "\n",
            "112:\tloss: 0.2487082\tbest: 0.1510048 (98)\ttotal: 9m 33s\tremaining: 2m 37s\n",
            "\n",
            "bestTest = 0.2166487162\n",
            "bestIteration = 99\n",
            "\n",
            "113:\tloss: 0.2166487\tbest: 0.1510048 (98)\ttotal: 9m 41s\tremaining: 2m 33s\n",
            "\n",
            "bestTest = 0.3660449009\n",
            "bestIteration = 99\n",
            "\n",
            "114:\tloss: 0.3660449\tbest: 0.1510048 (98)\ttotal: 9m 48s\tremaining: 2m 28s\n",
            "\n",
            "bestTest = 0.2506371095\n",
            "bestIteration = 99\n",
            "\n",
            "115:\tloss: 0.2506371\tbest: 0.1510048 (98)\ttotal: 9m 56s\tremaining: 2m 23s\n",
            "\n",
            "bestTest = 0.2160656619\n",
            "bestIteration = 99\n",
            "\n",
            "116:\tloss: 0.2160657\tbest: 0.1510048 (98)\ttotal: 10m 2s\tremaining: 2m 19s\n",
            "\n",
            "bestTest = 0.368852877\n",
            "bestIteration = 99\n",
            "\n",
            "117:\tloss: 0.3688529\tbest: 0.1510048 (98)\ttotal: 10m 10s\tremaining: 2m 14s\n",
            "\n",
            "bestTest = 0.2521438255\n",
            "bestIteration = 99\n",
            "\n",
            "118:\tloss: 0.2521438\tbest: 0.1510048 (98)\ttotal: 10m 17s\tremaining: 2m 9s\n",
            "\n",
            "bestTest = 0.2189416613\n",
            "bestIteration = 99\n",
            "\n",
            "119:\tloss: 0.2189417\tbest: 0.1510048 (98)\ttotal: 10m 25s\tremaining: 2m 5s\n",
            "\n",
            "bestTest = 0.2957890729\n",
            "bestIteration = 199\n",
            "\n",
            "120:\tloss: 0.2957891\tbest: 0.1510048 (98)\ttotal: 10m 40s\tremaining: 2m 1s\n",
            "\n",
            "bestTest = 0.2083335432\n",
            "bestIteration = 199\n",
            "\n",
            "121:\tloss: 0.2083335\tbest: 0.1510048 (98)\ttotal: 10m 55s\tremaining: 1m 58s\n",
            "\n",
            "bestTest = 0.1712107893\n",
            "bestIteration = 199\n",
            "\n",
            "122:\tloss: 0.1712108\tbest: 0.1510048 (98)\ttotal: 11m 10s\tremaining: 1m 54s\n",
            "\n",
            "bestTest = 0.3005764143\n",
            "bestIteration = 199\n",
            "\n",
            "123:\tloss: 0.3005764\tbest: 0.1510048 (98)\ttotal: 11m 25s\tremaining: 1m 50s\n",
            "\n",
            "bestTest = 0.2146600462\n",
            "bestIteration = 199\n",
            "\n",
            "124:\tloss: 0.2146600\tbest: 0.1510048 (98)\ttotal: 11m 40s\tremaining: 1m 46s\n",
            "\n",
            "bestTest = 0.1796315751\n",
            "bestIteration = 199\n",
            "\n",
            "125:\tloss: 0.1796316\tbest: 0.1510048 (98)\ttotal: 11m 57s\tremaining: 1m 42s\n",
            "\n",
            "bestTest = 0.302821926\n",
            "bestIteration = 199\n",
            "\n",
            "126:\tloss: 0.3028219\tbest: 0.1510048 (98)\ttotal: 12m 12s\tremaining: 1m 38s\n",
            "\n",
            "bestTest = 0.2179767386\n",
            "bestIteration = 199\n",
            "\n",
            "127:\tloss: 0.2179767\tbest: 0.1510048 (98)\ttotal: 12m 27s\tremaining: 1m 33s\n",
            "\n",
            "bestTest = 0.1826269327\n",
            "bestIteration = 199\n",
            "\n",
            "128:\tloss: 0.1826269\tbest: 0.1510048 (98)\ttotal: 12m 42s\tremaining: 1m 28s\n",
            "\n",
            "bestTest = 0.3056754705\n",
            "bestIteration = 199\n",
            "\n",
            "129:\tloss: 0.3056755\tbest: 0.1510048 (98)\ttotal: 12m 57s\tremaining: 1m 23s\n",
            "\n",
            "bestTest = 0.2189960842\n",
            "bestIteration = 199\n",
            "\n",
            "130:\tloss: 0.2189961\tbest: 0.1510048 (98)\ttotal: 13m 12s\tremaining: 1m 18s\n",
            "\n",
            "bestTest = 0.185871193\n",
            "bestIteration = 199\n",
            "\n",
            "131:\tloss: 0.1858712\tbest: 0.1510048 (98)\ttotal: 13m 26s\tremaining: 1m 13s\n",
            "\n",
            "bestTest = 0.243532456\n",
            "bestIteration = 499\n",
            "\n",
            "132:\tloss: 0.2435325\tbest: 0.1510048 (98)\ttotal: 14m 4s\tremaining: 1m 9s\n",
            "\n",
            "bestTest = 0.1608933408\n",
            "bestIteration = 499\n",
            "\n",
            "133:\tloss: 0.1608933\tbest: 0.1510048 (98)\ttotal: 14m 40s\tremaining: 1m 5s\n",
            "\n",
            "bestTest = 0.1427830726\n",
            "bestIteration = 499\n",
            "\n",
            "134:\tloss: 0.1427831\tbest: 0.1427831 (134)\ttotal: 15m 17s\tremaining: 1m 1s\n",
            "\n",
            "bestTest = 0.2484113533\n",
            "bestIteration = 499\n",
            "\n",
            "135:\tloss: 0.2484114\tbest: 0.1427831 (134)\ttotal: 15m 55s\tremaining: 56.2s\n",
            "\n",
            "bestTest = 0.168484496\n",
            "bestIteration = 499\n",
            "\n",
            "136:\tloss: 0.1684845\tbest: 0.1427831 (134)\ttotal: 16m 32s\tremaining: 50.7s\n",
            "\n",
            "bestTest = 0.1488956066\n",
            "bestIteration = 499\n",
            "\n",
            "137:\tloss: 0.1488956\tbest: 0.1427831 (134)\ttotal: 17m 8s\tremaining: 44.7s\n",
            "\n",
            "bestTest = 0.2505003763\n",
            "bestIteration = 499\n",
            "\n",
            "138:\tloss: 0.2505004\tbest: 0.1427831 (134)\ttotal: 17m 46s\tremaining: 38.3s\n",
            "\n",
            "bestTest = 0.1733221412\n",
            "bestIteration = 499\n",
            "\n",
            "139:\tloss: 0.1733221\tbest: 0.1427831 (134)\ttotal: 18m 23s\tremaining: 31.5s\n",
            "\n",
            "bestTest = 0.1518690918\n",
            "bestIteration = 499\n",
            "\n",
            "140:\tloss: 0.1518691\tbest: 0.1427831 (134)\ttotal: 19m\tremaining: 24.3s\n",
            "\n",
            "bestTest = 0.2529760387\n",
            "bestIteration = 499\n",
            "\n",
            "141:\tloss: 0.2529760\tbest: 0.1427831 (134)\ttotal: 19m 37s\tremaining: 16.6s\n",
            "\n",
            "bestTest = 0.1771249027\n",
            "bestIteration = 499\n",
            "\n",
            "142:\tloss: 0.1771249\tbest: 0.1427831 (134)\ttotal: 20m 14s\tremaining: 8.49s\n",
            "\n",
            "bestTest = 0.1535697305\n",
            "bestIteration = 499\n",
            "\n",
            "143:\tloss: 0.1535697\tbest: 0.1427831 (134)\ttotal: 20m 51s\tremaining: 0us\n",
            "Estimating final quality...\n",
            "Training on fold [0/5]\n",
            "\n",
            "bestTest = 0.1544310359\n",
            "bestIteration = 499\n",
            "\n",
            "Training on fold [1/5]\n",
            "\n",
            "bestTest = 0.1495403495\n",
            "bestIteration = 498\n",
            "\n",
            "Training on fold [2/5]\n",
            "\n",
            "bestTest = 0.1473018192\n",
            "bestIteration = 499\n",
            "\n",
            "Training on fold [3/5]\n",
            "\n",
            "bestTest = 0.1416694826\n",
            "bestIteration = 498\n",
            "\n",
            "Training on fold [4/5]\n",
            "\n",
            "bestTest = 0.1418716151\n",
            "bestIteration = 499\n",
            "\n",
            "CatBoost Best Parameters: {'depth': 10, 'learning_rate': 0.1, 'l2_leaf_reg': 1, 'iterations': 500}\n",
            "CatBoost Accuracy: 0.9414\n",
            "CatBoost F1-Score: 0.9428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGB\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "}\n",
        "\n",
        "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "random_search_xgb = RandomizedSearchCV(estimator=xgb, param_distributions=param_dist, n_iter=20, cv=5, scoring='accuracy', verbose=2, random_state=42)\n",
        "random_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# 최적 모델로 테스트 데이터 평가\n",
        "best_xgb = random_search_xgb.best_estimator_\n",
        "y_pred_xgb = best_xgb.predict(X_test)\n",
        "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
        "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"XGBoost Best Parameters:\", random_search_xgb.best_params_)\n",
        "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
        "print(f\"XGBoost F1-Score: {xgb_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GvEoypwDGq0",
        "outputId": "f286eed0-1d75-4559-87a5-e27185473716"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=3, n_estimators=500, subsample=0.8; total time=   3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=3, n_estimators=500, subsample=0.8; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=3, n_estimators=500, subsample=0.8; total time=   6.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=3, n_estimators=500, subsample=0.8; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=3, n_estimators=500, subsample=0.8; total time=  13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.6; total time=   1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:47:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.6; total time=   1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.6; total time=   1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.6; total time=   4.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.6; total time=   1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=500, subsample=0.6; total time=   2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:09] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=500, subsample=0.6; total time=   2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=500, subsample=0.6; total time=   2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=500, subsample=0.6; total time=   4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=500, subsample=0.6; total time=   2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, subsample=0.8; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, subsample=0.8; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, subsample=0.8; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, subsample=0.8; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=100, subsample=0.8; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=500, subsample=0.8; total time=   7.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=500, subsample=0.8; total time=   4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=500, subsample=0.8; total time=   7.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=500, subsample=0.8; total time=   4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:48:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=500, subsample=0.8; total time=   7.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, subsample=1.0; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, subsample=1.0; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, subsample=1.0; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, subsample=1.0; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, min_child_weight=5, n_estimators=500, subsample=1.0; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, subsample=0.8; total time=  12.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   1.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   4.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=500, subsample=0.8; total time=  19.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=500, subsample=0.8; total time=   6.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=500, subsample=0.8; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:49:58] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=500, subsample=0.8; total time=  20.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:50:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=500, subsample=0.8; total time=   4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:50:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, subsample=0.6; total time=   1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:50:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, subsample=0.6; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:50:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, subsample=0.6; total time=   9.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:50:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, subsample=0.6; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:50:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, min_child_weight=5, n_estimators=500, subsample=0.6; total time=  14.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, min_child_weight=5, n_estimators=200, subsample=0.8; total time=   1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.6; total time=   0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.6; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.6; total time=   0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.6; total time=   1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.6; total time=   1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, min_child_weight=5, n_estimators=100, subsample=0.8; total time=   1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, min_child_weight=5, n_estimators=100, subsample=0.8; total time=   1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, min_child_weight=5, n_estimators=100, subsample=0.8; total time=   1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, min_child_weight=5, n_estimators=100, subsample=0.8; total time=   1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, min_child_weight=5, n_estimators=100, subsample=0.8; total time=   4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=500, subsample=1.0; total time=   1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=500, subsample=1.0; total time=   1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=500, subsample=1.0; total time=   1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=500, subsample=1.0; total time=   1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=500, subsample=1.0; total time=   1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=500, subsample=0.8; total time=   1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=500, subsample=0.8; total time=   3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=500, subsample=0.8; total time=   1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=500, subsample=0.8; total time=   1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=500, subsample=0.8; total time=   1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.6; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.6; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.6; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.6; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.6; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:51:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Best Parameters: {'subsample': 0.8, 'n_estimators': 500, 'min_child_weight': 1, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
            "XGBoost Accuracy: 0.9435\n",
            "XGBoost F1-Score: 0.9446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#앙상블 기법"
      ],
      "metadata": {
        "id": "IavPGolnOuus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import StackingClassifier"
      ],
      "metadata": {
        "id": "HqxfNYoROx-U"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {\n",
        "    \"LightGBM\": {\"learning_rate\": 0.2, \"n_estimators\": 500, \"num_leaves\": 40, \"max_depth\": -1},\n",
        "    \"CatBoost\": {\"iterations\": 200, \"learning_rate\": 0.1, \"depth\": 10, 'l2_leaf_reg': 1},\n",
        "    \"XGBoost\": {\"learning_rate\": 0.1, \"n_estimators\": 500, \"max_depth\": 10, \"min_child_weight\": 1, \"subsample\": 0.8, \"colsample_bytree\": 0.8},\n",
        "}"
      ],
      "metadata": {
        "id": "WcV2eR0BPQ3-"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"lgbm\": LGBMClassifier(**best_params[\"LightGBM\"], random_state=42),\n",
        "    \"catboost\": CatBoostClassifier(**best_params[\"CatBoost\"], random_state=42, verbose=0),\n",
        "    \"xgb\": XGBClassifier(**best_params[\"XGBoost\"], random_state=42, use_label_encoder=False, eval_metric=\"logloss\"),\n",
        "}"
      ],
      "metadata": {
        "id": "zSmi7IvDPgIu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Votingclassifier 사용\n",
        "\n",
        "VotingClassifier는 여러 모델의 예측 결과를 결합하여 최종 예측을 수행합니다. Hard Voting(다수결) 또는 Soft Voting(확률 기반 결합)을 선택할 수 있습니다."
      ],
      "metadata": {
        "id": "x11_MKA2PzoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VotingClassifier 정의 (Soft Voting)\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lgbm', lgbm),\n",
        "        ('catboost', catboost),\n",
        "        ('xgb', xgb)\n",
        "    ],\n",
        "    voting='soft'  # 'soft': 확률 기반, 'hard': 다수결\n",
        ")"
      ],
      "metadata": {
        "id": "gTVTGaG8PokH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# 예측 및 평가\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"VotingClassifier Accuracy: {accuracy:.4f}\")\n",
        "print(f\"VotingClassifier F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOfu9IbpPqlQ",
        "outputId": "eae21b61-9aa0-4434-aeac-72e326c4548f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 13310, number of negative: 13290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003279 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 26600, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:07:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VotingClassifier Accuracy: 0.9368\n",
            "VotingClassifier F1-Score: 0.9388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- StackingClassifier 사용\n",
        "\n",
        "StackingClassifier는 여러 모델의 예측 결과를 새로운 모델의 입력으로 사용하여 최종 예측을 수행합니다."
      ],
      "metadata": {
        "id": "379LYwiHP7JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "# StackingClassifier 정의\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('lgbm', lgbm),\n",
        "        ('catboost', catboost),\n",
        "        ('xgb', xgb),\n",
        "        ('ada', ada)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(),  # 최종 조합 모델\n",
        "    cv=5  # 내부 교차 검증\n",
        ")\n",
        "\n",
        "# 앙상블 모델 학습\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# 예측 및 평가\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"StackingClassifier Accuracy: {accuracy:.4f}\")\n",
        "print(f\"StackingClassifier F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZtuvQ4uQFGZ",
        "outputId": "988bf83f-86ad-49ef-eec0-01a312f63a01"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 13310, number of negative: 13290\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001551 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 26600, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:36:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003660 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003694 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003587 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006064 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003716 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:40:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:40:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:40:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:40:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:40:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StackingClassifier Accuracy: 0.9427\n",
            "StackingClassifier F1-Score: 0.9436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#최종 모델로 gradient Boosting 사\n",
        "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier\n",
        "\n",
        "# StackingClassifier 정의\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('lgbm', lgbm),\n",
        "        ('catboost', catboost),\n",
        "        ('xgb', xgb),\n",
        "        ('ada', ada)\n",
        "    ],\n",
        "    final_estimator= GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=3, random_state=42),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# 앙상블 모델 학습\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# 예측 및 평가\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"StackingClassifier Accuracy: {accuracy:.4f}\")\n",
        "print(f\"StackingClassifier F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUkibMgURo9C",
        "outputId": "bc8d7a51-49d9-4b22-9e44-ec2d63289508"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 13310, number of negative: 13290\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004524 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 26600, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:41:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005180 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006031 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003890 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003603 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 10648, number of negative: 10632\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003848 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 21280, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500376 -> initscore=0.001504\n",
            "[LightGBM] [Info] Start training from score 0.001504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:44:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StackingClassifier Accuracy: 0.9447\n",
            "StackingClassifier F1-Score: 0.9455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#교차 검증"
      ],
      "metadata": {
        "id": "HBMFiusWDwJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import make_scorer"
      ],
      "metadata": {
        "id": "aeorVcnfLVU0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {\n",
        "    \"AdaBoost\": {\"n_estimators\": 200, \"learning_rate\": 0.1},\n",
        "    \"LightGBM\": {\"learning_rate\": 0.2, \"n_estimators\": 500, \"num_leaves\": 40, \"max_depth\": -1},\n",
        "    \"CatBoost\": {\"iterations\": 200, \"learning_rate\": 0.1, \"depth\": 10, 'l2_leaf_reg': 1},\n",
        "    \"XGBoost\": {\"learning_rate\": 0.1, \"n_estimators\": 500, \"max_depth\": 10, \"min_child_weight\": 1, \"subsample\": 0.8, \"colsample_bytree\": 0.8},\n",
        "}"
      ],
      "metadata": {
        "id": "5y7wrqbFG5f2"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"AdaBoost\": AdaBoostClassifier(**best_params[\"AdaBoost\"], random_state=42),\n",
        "    \"LightGBM\": LGBMClassifier(**best_params[\"LightGBM\"], random_state=42),\n",
        "    \"CatBoost\": CatBoostClassifier(**best_params[\"CatBoost\"], random_state=42, verbose=0),\n",
        "    \"XGBoost\": XGBClassifier(**best_params[\"XGBoost\"], random_state=42, use_label_encoder=False, eval_metric=\"logloss\"),\n",
        "}"
      ],
      "metadata": {
        "id": "fXX4BXvvG8e5"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score"
      ],
      "metadata": {
        "id": "00u-a-d-L9NE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# K-Fold 설정\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Scorer 설정\n",
        "f1_scorer = make_scorer(f1_score)\n",
        "\n",
        "# 교차 검증 실행\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "    accuracy_scores = cross_val_score(model, X_resampled, y_resampled, cv=kfold, scoring='accuracy')\n",
        "    f1_scores = cross_val_score(model, X_resampled, y_resampled, cv=kfold, scoring=f1_scorer)\n",
        "\n",
        "    results[model_name] = {\n",
        "        \"Accuracy (mean)\": accuracy_scores.mean(),\n",
        "        \"Accuracy (std)\": accuracy_scores.std(),\n",
        "        \"F1-Score (mean)\": f1_scores.mean(),\n",
        "        \"F1-Score (std)\": f1_scores.std(),\n",
        "    }\n",
        "\n",
        "for model_name, scores in results.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"  Mean Accuracy: {scores['Accuracy (mean)']:.4f} (±{scores['Accuracy (std)']:.4f})\")\n",
        "    print(f\"  Mean F1-Score: {scores['F1-Score (mean)']:.4f} (±{scores['F1-Score (std)']:.4f})\")"
      ],
      "metadata": {
        "id": "BQHVd6QFDthY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e78eee6b-8696-478f-fae3-469d4c594465"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating lgbm...\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 15200, number of negative: 15200\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016101 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 30400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 15200, number of negative: 15200\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008656 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 30400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 15200, number of negative: 15200\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008541 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 30400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 15200, number of negative: 15200\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011126 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 30400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 15200, number of negative: 15200\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008691 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 30400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 15200, number of negative: 15200\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008662 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 30400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 15200, number of negative: 15200\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008607 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 30400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 15200, number of negative: 15200\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004923 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 30400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 15200, number of negative: 15200\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004916 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 30400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 15200, number of negative: 15200\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008528 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 30400, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "\n",
            "Evaluating catboost...\n",
            "\n",
            "Evaluating xgb...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:28:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:29:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:29:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:29:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:29:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:29:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:29:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:29:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:29:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [07:29:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: lgbm\n",
            "  Mean Accuracy: 0.9479 (±0.0019)\n",
            "  Mean F1-Score: 0.9490 (±0.0018)\n",
            "\n",
            "Model: catboost\n",
            "  Mean Accuracy: 0.9333 (±0.0028)\n",
            "  Mean F1-Score: 0.9354 (±0.0025)\n",
            "\n",
            "Model: xgb\n",
            "  Mean Accuracy: 0.9486 (±0.0022)\n",
            "  Mean F1-Score: 0.9496 (±0.0021)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dR1xsIpdO4Ie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}