{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12E6ICnwDsFHkGuy--LAvDXeiThTfV5Py",
      "authorship_tag": "ABX9TyObcBGQLvKfxazlcH1AGx1p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindyshin2211/Website_Fingerprinting_MLB/blob/%EC%8B%A0%EC%84%B1%ED%98%84/XGBoost_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "mon=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/EWHA_machine_ learning/mon.csv')\n",
        "\n",
        "# 특징(X)와 타겟(y) 분리\n",
        "X = mon.drop(columns=['Label'])\n",
        "y = mon['Label']\n",
        "\n",
        "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGBoost 하이퍼파라미터 튜닝\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', verbose=0)\n",
        "\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='f1_weighted', cv=3, verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 하이퍼파라미터 출력\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 튜닝된 모델로 예측\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "xgb_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# F1 Score 및 정확도 출력\n",
        "xgb_f1 = f1_score(y_test, xgb_pred, average='weighted')\n",
        "print(\"Tuned XGBoost Accuracy:\", accuracy_score(y_test, xgb_pred))\n",
        "print(\"Tuned XGBoost F1 Score (weighted):\", xgb_f1)\n",
        "print(\"\\nClassification Report (Tuned XGBoost):\\n\", classification_report(y_test, xgb_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5aDk3uyN6Nf",
        "outputId": "df638f97-efcf-43f2-864c-d8490bc01e4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [09:33:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\", \"verbose\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Tuned XGBoost Accuracy: 0.7794736842105263\n",
            "Tuned XGBoost F1 Score (weighted): 0.7791529577062408\n",
            "\n",
            "Classification Report (Tuned XGBoost):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.62      0.65        42\n",
            "           1       0.84      0.76      0.80        42\n",
            "           2       0.85      0.94      0.89        35\n",
            "           3       0.79      0.93      0.86        29\n",
            "           4       0.78      0.90      0.83        39\n",
            "           5       0.91      0.87      0.89        45\n",
            "           6       0.85      0.91      0.88        44\n",
            "           7       0.78      0.78      0.78        36\n",
            "           8       0.82      0.68      0.74        34\n",
            "           9       0.65      0.71      0.68        31\n",
            "          10       0.90      0.79      0.84        47\n",
            "          11       0.69      0.77      0.73        35\n",
            "          12       0.90      0.83      0.86        42\n",
            "          13       0.57      0.53      0.55        40\n",
            "          14       0.68      0.58      0.63        36\n",
            "          15       0.72      0.74      0.73        35\n",
            "          16       0.89      0.74      0.81        43\n",
            "          17       0.75      0.85      0.80        47\n",
            "          18       0.94      0.84      0.89        37\n",
            "          19       0.78      0.68      0.72        37\n",
            "          20       0.87      0.91      0.89        44\n",
            "          21       0.71      0.71      0.71        41\n",
            "          22       0.74      0.65      0.69        40\n",
            "          23       0.74      0.83      0.78        35\n",
            "          24       0.49      0.46      0.47        46\n",
            "          25       0.78      0.69      0.74        36\n",
            "          26       0.83      0.95      0.89        37\n",
            "          27       0.89      0.76      0.82        45\n",
            "          28       0.86      0.83      0.85        36\n",
            "          29       0.73      0.79      0.76        47\n",
            "          30       0.78      0.87      0.82        46\n",
            "          31       0.67      0.86      0.75        37\n",
            "          32       0.74      0.66      0.69        38\n",
            "          33       0.79      0.92      0.85        37\n",
            "          34       0.57      0.63      0.60        41\n",
            "          35       0.78      0.82      0.79        38\n",
            "          36       0.86      0.89      0.87        35\n",
            "          37       0.76      0.58      0.66        53\n",
            "          38       0.72      0.64      0.68        53\n",
            "          39       0.91      0.70      0.79        46\n",
            "          40       0.75      0.76      0.76        51\n",
            "          41       0.88      0.86      0.87        43\n",
            "          42       0.68      0.67      0.68        39\n",
            "          43       0.95      0.88      0.91        41\n",
            "          44       0.91      0.95      0.93        44\n",
            "          45       0.54      0.63      0.58        35\n",
            "          46       0.79      0.74      0.76        35\n",
            "          47       0.68      0.67      0.68        39\n",
            "          48       0.76      0.86      0.81        37\n",
            "          49       0.91      0.70      0.79        43\n",
            "          50       0.76      0.88      0.81        32\n",
            "          51       0.58      0.74      0.65        38\n",
            "          52       0.86      0.88      0.87        49\n",
            "          53       0.73      0.87      0.79        31\n",
            "          54       0.88      0.81      0.84        47\n",
            "          55       0.71      0.71      0.71        41\n",
            "          56       0.96      0.94      0.95        47\n",
            "          57       0.80      0.89      0.84        36\n",
            "          58       0.91      0.91      0.91        35\n",
            "          59       0.95      0.90      0.92        39\n",
            "          60       0.85      0.88      0.86        40\n",
            "          61       0.83      0.68      0.75        56\n",
            "          62       0.79      0.72      0.75        57\n",
            "          63       0.57      0.74      0.65        27\n",
            "          64       0.64      0.77      0.70        39\n",
            "          65       0.58      0.63      0.60        35\n",
            "          66       0.86      0.83      0.84        46\n",
            "          67       0.80      0.89      0.85        37\n",
            "          68       0.97      0.60      0.74        48\n",
            "          69       0.76      0.81      0.79        43\n",
            "          70       0.91      0.88      0.90        34\n",
            "          71       0.71      0.83      0.77        36\n",
            "          72       0.80      0.82      0.81        39\n",
            "          73       0.86      0.90      0.88        41\n",
            "          74       0.79      0.64      0.71        53\n",
            "          75       0.85      0.92      0.88        37\n",
            "          76       0.89      0.95      0.92        44\n",
            "          77       0.61      0.67      0.63        30\n",
            "          78       0.57      0.81      0.67        36\n",
            "          79       0.67      0.67      0.67        30\n",
            "          80       0.88      0.93      0.90        40\n",
            "          81       0.77      0.80      0.78        45\n",
            "          82       0.68      0.56      0.61        45\n",
            "          83       0.65      0.90      0.76        31\n",
            "          84       0.91      0.75      0.82        40\n",
            "          85       0.95      0.89      0.92        44\n",
            "          86       0.95      0.97      0.96        37\n",
            "          87       0.90      0.80      0.85        35\n",
            "          88       0.73      0.73      0.73        37\n",
            "          89       0.74      0.66      0.69        38\n",
            "          90       0.84      0.86      0.85        36\n",
            "          91       0.74      0.74      0.74        39\n",
            "          92       0.68      0.77      0.72        35\n",
            "          93       0.92      0.92      0.92        39\n",
            "          94       0.61      0.60      0.60        42\n",
            "\n",
            "    accuracy                           0.78      3800\n",
            "   macro avg       0.78      0.78      0.78      3800\n",
            "weighted avg       0.79      0.78      0.78      3800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UMKSnzagPlr5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}