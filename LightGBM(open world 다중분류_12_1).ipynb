{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18752,"status":"ok","timestamp":1732982005313,"user":{"displayName":"오윤재","userId":"04375251631092764589"},"user_tz":-540},"id":"LQhjHppc9pJR","outputId":"0ffcbd38-8004-45ff-f183-b13dac0ed710"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtOOy0ZsXHAz"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChMhlMzyXe1S"},"outputs":[],"source":["data=pd.read_csv('/content/drive/MyDrive/monunmon.csv')"]},{"cell_type":"markdown","metadata":{"id":"wDy0xdi-ZDUU"},"source":["**Light GBM**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IZTqcVXya0uT"},"source":["- 기본 모델 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51390,"status":"ok","timestamp":1732983749989,"user":{"displayName":"오윤재","userId":"04375251631092764589"},"user_tz":-540},"id":"jDhxA60F8Ukt","outputId":"bf0bb0bb-39a3-4326-904d-019f65e731d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.7095454545454546\n","Test Accuracy: 0.7227272727272728\n","Test F1 Score (weighted): 0.7286673447790059\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","          -1       0.46      0.81      0.59       600\n","           0       0.83      0.62      0.71        40\n","           1       0.71      0.55      0.62        40\n","           2       0.87      0.82      0.85        40\n","           3       0.78      0.72      0.75        40\n","           4       0.83      0.62      0.71        40\n","           5       0.82      0.70      0.76        40\n","           6       0.77      0.85      0.81        40\n","           7       0.90      0.68      0.77        40\n","           8       0.80      0.60      0.69        40\n","           9       0.66      0.57      0.61        40\n","          10       0.78      0.62      0.69        40\n","          11       0.77      0.68      0.72        40\n","          12       0.88      0.88      0.88        40\n","          13       0.71      0.50      0.59        40\n","          14       0.71      0.68      0.69        40\n","          15       0.78      0.62      0.69        40\n","          16       0.89      0.60      0.72        40\n","          17       0.84      0.68      0.75        40\n","          18       0.97      0.88      0.92        40\n","          19       0.97      0.78      0.86        40\n","          20       1.00      0.95      0.97        40\n","          21       0.83      0.60      0.70        40\n","          22       0.70      0.57      0.63        40\n","          23       0.89      0.80      0.84        40\n","          24       0.41      0.42      0.42        40\n","          25       0.69      0.60      0.64        40\n","          26       0.77      0.82      0.80        40\n","          27       0.78      0.88      0.82        40\n","          28       0.94      0.75      0.83        40\n","          29       0.73      0.68      0.70        40\n","          30       0.92      0.90      0.91        40\n","          31       0.76      0.85      0.80        40\n","          32       0.70      0.57      0.63        40\n","          33       1.00      0.75      0.86        40\n","          34       0.61      0.57      0.59        40\n","          35       0.95      0.90      0.92        40\n","          36       0.80      0.82      0.81        40\n","          37       0.71      0.42      0.53        40\n","          38       0.71      0.68      0.69        40\n","          39       0.89      0.80      0.84        40\n","          40       0.81      0.65      0.72        40\n","          41       0.83      0.88      0.85        40\n","          42       0.76      0.70      0.73        40\n","          43       0.82      0.68      0.74        40\n","          44       0.92      0.90      0.91        40\n","          45       0.74      0.62      0.68        40\n","          46       0.85      0.55      0.67        40\n","          47       0.67      0.50      0.57        40\n","          48       0.91      0.72      0.81        40\n","          49       0.94      0.75      0.83        40\n","          50       0.84      0.78      0.81        40\n","          51       0.69      0.50      0.58        40\n","          52       0.81      0.75      0.78        40\n","          53       0.86      0.60      0.71        40\n","          54       0.86      0.78      0.82        40\n","          55       0.61      0.55      0.58        40\n","          56       0.95      0.93      0.94        40\n","          57       0.83      0.75      0.79        40\n","          58       0.87      0.82      0.85        40\n","          59       1.00      0.78      0.87        40\n","          60       0.91      0.78      0.84        40\n","          61       0.80      0.60      0.69        40\n","          62       0.83      0.60      0.70        40\n","          63       0.72      0.57      0.64        40\n","          64       0.81      0.65      0.72        40\n","          65       0.87      0.68      0.76        40\n","          66       0.79      0.82      0.80        40\n","          67       0.78      0.72      0.75        40\n","          68       0.81      0.55      0.66        40\n","          69       0.56      0.55      0.56        40\n","          70       0.93      0.93      0.93        40\n","          71       0.72      0.85      0.78        40\n","          72       0.76      0.70      0.73        40\n","          73       0.97      0.95      0.96        40\n","          74       0.69      0.60      0.64        40\n","          75       0.95      0.93      0.94        40\n","          76       0.93      0.93      0.93        40\n","          77       0.61      0.57      0.59        40\n","          78       0.71      0.80      0.75        40\n","          79       0.67      0.65      0.66        40\n","          80       0.88      0.88      0.88        40\n","          81       0.76      0.78      0.77        40\n","          82       0.74      0.70      0.72        40\n","          83       0.85      0.82      0.84        40\n","          84       0.86      0.78      0.82        40\n","          85       0.86      0.75      0.80        40\n","          86       0.97      0.93      0.95        40\n","          87       0.88      0.75      0.81        40\n","          88       0.78      0.78      0.78        40\n","          89       0.56      0.45      0.50        40\n","          90       0.90      0.65      0.75        40\n","          91       0.83      0.60      0.70        40\n","          92       0.73      0.47      0.58        40\n","          93       0.90      0.93      0.91        40\n","          94       0.67      0.55      0.60        40\n","\n","    accuracy                           0.72      4400\n","   macro avg       0.80      0.71      0.75      4400\n","weighted avg       0.76      0.72      0.73      4400\n","\n","\n","Confusion Matrix:\n"," [[484   0   4 ...   3   1   1]\n"," [  8  25   0 ...   0   0   0]\n"," [ 11   0  22 ...   0   0   0]\n"," ...\n"," [ 16   0   0 ...  19   0   0]\n"," [  1   0   0 ...   0  37   0]\n"," [  5   0   0 ...   0   0  22]]\n"]}],"source":["import lightgbm as lgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# 특징(X)와 타겟(y) 분리\n","X = data.drop(columns=['Label'])\n","y = data['Label']\n","\n","# 라벨 값 변환: -1 → 0, 0 → 1, ..., 94 → 95\n","y_transformed = y + 1\n","\n","# 데이터를 훈련 세트, 검증 세트, 테스트 세트로 분할 (60% train, 20% validation, 20% test)\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y_transformed, test_size=0.4, random_state=42, stratify=y_transformed)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n","\n","# LightGBM 데이터셋 생성\n","train_data = lgb.Dataset(X_train, label=y_train)\n","val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n","test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n","\n","# 모델 파라미터 설정\n","params = {\n","    'objective': 'multiclass',\n","    'num_class': len(np.unique(y_transformed)),  # 클래스 수\n","    'boosting_type': 'gbdt',\n","    'metric': 'multi_logloss',\n","    'is_unbalance': True,  # 불균형 데이터 대응\n","    'random_state': 42,\n","    'verbose': -1  # 로그 최소화\n","}\n","\n","# 모델 학습\n","model = lgb.train(\n","    params,\n","    train_data,\n","    valid_sets=[train_data, val_data]\n",")\n","\n","# Validation Accuracy 출력\n","y_val_pred_prob = model.predict(X_val)  # 검증 데이터에 대한 예측 확률\n","y_val_pred = np.argmax(y_val_pred_prob, axis=1)  # 가장 높은 확률의 클래스 선택\n","valid_accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation Accuracy: {valid_accuracy}\")\n","\n","# 테스트 세트 예측\n","y_test_pred_prob = model.predict(X_test)  # 테스트 데이터에 대한 예측 확률\n","y_test_pred = np.argmax(y_test_pred_prob, axis=1)  # 가장 높은 확률의 클래스 선택\n","\n","# 라벨 값 복원: 0 → -1, 1 → 0, ..., 95 → 94\n","y_test_original = y_test - 1\n","y_test_pred_original = y_test_pred - 1\n","\n","# 테스트 세트 성능 평가\n","test_accuracy = accuracy_score(y_test_original, y_test_pred_original)\n","classification_rep = classification_report(y_test_original, y_test_pred_original)\n","conf_matrix = confusion_matrix(y_test_original, y_test_pred_original)\n","test_f1_score = f1_score(y_test_original, y_test_pred_original, average='weighted')  # 가중 평균 F1 Score 계산\n","\n","print(f\"Test Accuracy: {test_accuracy}\")\n","print(f\"Test F1 Score (weighted): {test_f1_score}\")\n","print(\"\\nClassification Report:\\n\", classification_rep)\n","print(\"\\nConfusion Matrix:\\n\", conf_matrix)"]},{"cell_type":"markdown","source":["**weighted**"],"metadata":{"id":"qkDbHA9P5ifZ"}},{"cell_type":"code","source":["import lightgbm as lgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# 특징(X)와 타겟(y) 분리\n","X = data.drop(columns=['Label'])\n","y = data['Label']\n","\n","# 라벨 값 변환: -1 → 0, 0 → 1, ..., 94 → 95\n","y_transformed = y + 1\n","\n","# 데이터를 훈련 세트, 검증 세트, 테스트 세트로 분할 (60% train, 20% validation, 20% test)\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y_transformed, test_size=0.4, random_state=42, stratify=y_transformed)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n","\n","# 클래스 가중치 계산\n","class_weights = {label: len(y_train) / (len(np.unique(y_train)) * sum(y_train == label)) for label in np.unique(y_train)}\n","\n","# 가중치를 학습 데이터에 적용\n","weights = np.array([class_weights[label] for label in y_train])\n","\n","# LightGBM 데이터셋 생성 (가중치 포함)\n","train_data = lgb.Dataset(X_train, label=y_train, weight=weights)\n","val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n","test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n","\n","# 모델 파라미터 설정\n","params = {\n","    'objective': 'multiclass',\n","    'num_class': len(np.unique(y_transformed)),  # 클래스 수\n","    'boosting_type': 'gbdt',\n","    'metric': 'multi_logloss',\n","    'is_unbalance': True,  # 불균형 데이터 대응\n","    'random_state': 42,\n","    'verbose': -1  # 로그 최소화\n","}\n","\n","# 모델 학습\n","model = lgb.train(\n","    params,\n","    train_data,\n","    valid_sets=[train_data, val_data]\n",")\n","\n","# Validation Accuracy 출력\n","y_val_pred_prob = model.predict(X_val)  # 검증 데이터에 대한 예측 확률\n","y_val_pred = np.argmax(y_val_pred_prob, axis=1)  # 가장 높은 확률의 클래스 선택\n","valid_accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation Accuracy: {valid_accuracy}\")\n","\n","# 테스트 세트 예측\n","y_test_pred_prob = model.predict(X_test)  # 테스트 데이터에 대한 예측 확률\n","y_test_pred = np.argmax(y_test_pred_prob, axis=1)  # 가장 높은 확률의 클래스 선택\n","\n","# 라벨 값 복원: 0 → -1, 1 → 0, ..., 95 → 94\n","y_test_original = y_test - 1\n","y_test_pred_original = y_test_pred - 1\n","\n","# 테스트 세트 성능 평가\n","test_accuracy = accuracy_score(y_test_original, y_test_pred_original)\n","classification_rep = classification_report(y_test_original, y_test_pred_original)\n","conf_matrix = confusion_matrix(y_test_original, y_test_pred_original)\n","test_f1_score = f1_score(y_test_original, y_test_pred_original, average='weighted')  # 가중 평균 F1 Score 계산\n","\n","print(f\"Test Accuracy: {test_accuracy}\")\n","print(f\"Test F1 Score (weighted): {test_f1_score}\")\n","print(\"\\nClassification Report:\\n\", classification_rep)\n","print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LvVINg-8-4A5","executionInfo":{"status":"ok","timestamp":1732985618322,"user_tz":-540,"elapsed":48557,"user":{"displayName":"오윤재","userId":"04375251631092764589"}},"outputId":"367dc6d6-b2ce-4824-9cd7-52164d9368cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.7188636363636364\n","Test Accuracy: 0.7293181818181819\n","Test F1 Score (weighted): 0.7320001442166222\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","          -1       0.53      0.72      0.61       600\n","           0       0.83      0.60      0.70        40\n","           1       0.65      0.55      0.59        40\n","           2       0.87      0.85      0.86        40\n","           3       0.73      0.82      0.78        40\n","           4       0.76      0.65      0.70        40\n","           5       0.81      0.75      0.78        40\n","           6       0.86      0.80      0.83        40\n","           7       0.88      0.72      0.79        40\n","           8       0.85      0.72      0.78        40\n","           9       0.62      0.65      0.63        40\n","          10       0.81      0.65      0.72        40\n","          11       0.81      0.65      0.72        40\n","          12       0.78      0.80      0.79        40\n","          13       0.68      0.47      0.56        40\n","          14       0.73      0.68      0.70        40\n","          15       0.91      0.75      0.82        40\n","          16       0.83      0.62      0.71        40\n","          17       0.74      0.65      0.69        40\n","          18       0.88      0.90      0.89        40\n","          19       0.97      0.80      0.88        40\n","          20       1.00      0.95      0.97        40\n","          21       0.71      0.62      0.67        40\n","          22       0.65      0.55      0.59        40\n","          23       0.76      0.80      0.78        40\n","          24       0.35      0.45      0.39        40\n","          25       0.69      0.72      0.71        40\n","          26       0.72      0.78      0.75        40\n","          27       0.78      0.90      0.84        40\n","          28       0.86      0.75      0.80        40\n","          29       0.73      0.68      0.70        40\n","          30       0.82      0.90      0.86        40\n","          31       0.73      0.90      0.81        40\n","          32       0.65      0.60      0.62        40\n","          33       0.88      0.72      0.79        40\n","          34       0.67      0.55      0.60        40\n","          35       0.92      0.90      0.91        40\n","          36       0.79      0.82      0.80        40\n","          37       0.78      0.53      0.63        40\n","          38       0.74      0.62      0.68        40\n","          39       0.76      0.80      0.78        40\n","          40       0.71      0.72      0.72        40\n","          41       0.88      0.90      0.89        40\n","          42       0.61      0.70      0.65        40\n","          43       0.76      0.72      0.74        40\n","          44       0.95      0.90      0.92        40\n","          45       0.68      0.68      0.68        40\n","          46       0.82      0.57      0.68        40\n","          47       0.73      0.60      0.66        40\n","          48       0.86      0.78      0.82        40\n","          49       0.89      0.80      0.84        40\n","          50       0.85      0.82      0.84        40\n","          51       0.69      0.50      0.58        40\n","          52       0.86      0.78      0.82        40\n","          53       0.87      0.65      0.74        40\n","          54       0.87      0.82      0.85        40\n","          55       0.47      0.57      0.52        40\n","          56       0.95      0.95      0.95        40\n","          57       0.82      0.82      0.82        40\n","          58       0.86      0.90      0.88        40\n","          59       0.97      0.80      0.88        40\n","          60       0.94      0.78      0.85        40\n","          61       0.76      0.72      0.74        40\n","          62       0.71      0.60      0.65        40\n","          63       0.72      0.65      0.68        40\n","          64       0.67      0.75      0.71        40\n","          65       0.78      0.72      0.75        40\n","          66       0.80      0.88      0.83        40\n","          67       0.76      0.72      0.74        40\n","          68       0.77      0.57      0.66        40\n","          69       0.60      0.60      0.60        40\n","          70       0.88      0.93      0.90        40\n","          71       0.77      0.82      0.80        40\n","          72       0.74      0.70      0.72        40\n","          73       0.97      0.97      0.97        40\n","          74       0.62      0.65      0.63        40\n","          75       0.88      0.90      0.89        40\n","          76       0.93      0.93      0.93        40\n","          77       0.61      0.68      0.64        40\n","          78       0.67      0.80      0.73        40\n","          79       0.61      0.57      0.59        40\n","          80       0.89      0.85      0.87        40\n","          81       0.82      0.82      0.82        40\n","          82       0.74      0.70      0.72        40\n","          83       0.84      0.80      0.82        40\n","          84       0.74      0.72      0.73        40\n","          85       0.89      0.80      0.84        40\n","          86       0.90      0.90      0.90        40\n","          87       0.86      0.78      0.82        40\n","          88       0.87      0.68      0.76        40\n","          89       0.49      0.42      0.45        40\n","          90       0.84      0.68      0.75        40\n","          91       0.77      0.60      0.68        40\n","          92       0.66      0.53      0.58        40\n","          93       0.86      0.93      0.89        40\n","          94       0.69      0.60      0.64        40\n","\n","    accuracy                           0.73      4400\n","   macro avg       0.78      0.73      0.75      4400\n","weighted avg       0.75      0.73      0.73      4400\n","\n","\n","Confusion Matrix:\n"," [[433   1   6 ...   2   1   1]\n"," [  7  24   0 ...   0   0   0]\n"," [  8   0  22 ...   0   0   0]\n"," ...\n"," [ 12   0   0 ...  21   0   1]\n"," [  1   0   0 ...   0  37   0]\n"," [  3   1   0 ...   2   0  24]]\n"]}]},{"cell_type":"markdown","source":["**SMOTE**"],"metadata":{"id":"xbXQbsW8AIlL"}},{"cell_type":"code","source":["import lightgbm as lgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n","from imblearn.over_sampling import SMOTE\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# 특징(X)와 타겟(y) 분리\n","X = data.drop(columns=['Label'])\n","y = data['Label']\n","\n","# 라벨 값 변환: -1 → 0, 0 → 1, ..., 94 → 95\n","y_transformed = y + 1\n","\n","# 데이터를 훈련 세트, 검증 세트, 테스트 세트로 분할 (60% train, 20% validation, 20% test)\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y_transformed, test_size=0.4, random_state=42, stratify=y_transformed)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n","\n","# SMOTE 오버샘플링 적용\n","smote = SMOTE(random_state=42)\n","X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n","\n","# LightGBM 데이터셋 생성\n","train_data_smote = lgb.Dataset(X_train_smote, label=y_train_smote)\n","val_data = lgb.Dataset(X_val, label=y_val, reference=train_data_smote)\n","test_data = lgb.Dataset(X_test, label=y_test, reference=train_data_smote)\n","\n","# 모델 파라미터 설정\n","params = {\n","    'objective': 'multiclass',\n","    'num_class': len(np.unique(y_transformed)),  # 클래스 수\n","    'boosting_type': 'gbdt',\n","    'metric': 'multi_logloss',\n","    'is_unbalance': True,  # 불균형 데이터 대응\n","    'random_state': 42,\n","    'verbose': -1  # 로그 최소화\n","}\n","\n","# 모델 학습\n","model_smote = lgb.train(\n","    params,\n","    train_data_smote,\n","    valid_sets=[train_data_smote, val_data]\n",")\n","\n","# Validation Accuracy 출력\n","y_val_pred_prob = model_smote.predict(X_val)  # 검증 데이터에 대한 예측 확률\n","y_val_pred = np.argmax(y_val_pred_prob, axis=1)  # 가장 높은 확률의 클래스 선택\n","valid_accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation Accuracy: {valid_accuracy}\")\n","\n","# 테스트 세트 예측\n","y_test_pred_prob = model_smote.predict(X_test)  # 테스트 데이터에 대한 예측 확률\n","y_test_pred = np.argmax(y_test_pred_prob, axis=1)  # 가장 높은 확률의 클래스 선택\n","\n","# 라벨 값 복원: 0 → -1, 1 → 0, ..., 95 → 94\n","y_test_original = y_test - 1\n","y_test_pred_original = y_test_pred - 1\n","\n","# 테스트 세트 성능 평가\n","test_accuracy = accuracy_score(y_test_original, y_test_pred_original)\n","classification_rep = classification_report(y_test_original, y_test_pred_original)\n","conf_matrix = confusion_matrix(y_test_original, y_test_pred_original)\n","test_f1_score = f1_score(y_test_original, y_test_pred_original, average='weighted')  # 가중 평균 F1 Score 계산\n","\n","print(f\"Test Accuracy: {test_accuracy}\")\n","print(f\"Test F1 Score (weighted): {test_f1_score}\")\n","print(\"\\nClassification Report:\\n\", classification_rep)\n","print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vpNberTrAH8_","executionInfo":{"status":"ok","timestamp":1732986147550,"user_tz":-540,"elapsed":220308,"user":{"displayName":"오윤재","userId":"04375251631092764589"}},"outputId":"3e3a3e67-48b4-4225-d324-d7ced968367d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.1315909090909091\n","Test Accuracy: 0.1390909090909091\n","Test F1 Score (weighted): 0.12861952418113048\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","          -1       0.44      0.04      0.07       600\n","           0       0.08      0.12      0.10        40\n","           1       0.00      0.00      0.00        40\n","           2       0.00      0.00      0.00        40\n","           3       0.10      0.30      0.15        40\n","           4       0.00      0.00      0.00        40\n","           5       0.00      0.00      0.00        40\n","           6       0.15      0.38      0.22        40\n","           7       0.10      0.10      0.10        40\n","           8       0.11      0.23      0.15        40\n","           9       0.08      0.15      0.10        40\n","          10       0.03      0.05      0.03        40\n","          11       0.00      0.00      0.00        40\n","          12       0.00      0.00      0.00        40\n","          13       0.03      0.03      0.03        40\n","          14       0.05      0.12      0.07        40\n","          15       0.24      0.10      0.14        40\n","          16       0.29      0.40      0.33        40\n","          17       0.17      0.03      0.04        40\n","          18       0.09      0.12      0.11        40\n","          19       0.24      0.17      0.20        40\n","          20       0.04      0.03      0.03        40\n","          21       0.56      0.12      0.20        40\n","          22       0.00      0.00      0.00        40\n","          23       0.16      0.10      0.12        40\n","          24       0.10      0.12      0.11        40\n","          25       0.00      0.00      0.00        40\n","          26       0.00      0.00      0.00        40\n","          27       0.88      0.17      0.29        40\n","          28       0.33      0.40      0.36        40\n","          29       0.39      0.35      0.37        40\n","          30       0.75      0.07      0.14        40\n","          31       0.08      0.05      0.06        40\n","          32       0.00      0.00      0.00        40\n","          33       0.10      0.30      0.15        40\n","          34       0.00      0.00      0.00        40\n","          35       0.00      0.00      0.00        40\n","          36       0.05      0.07      0.06        40\n","          37       0.07      0.03      0.04        40\n","          38       0.15      0.10      0.12        40\n","          39       0.03      0.07      0.04        40\n","          40       0.17      0.05      0.08        40\n","          41       0.00      0.00      0.00        40\n","          42       0.13      0.23      0.17        40\n","          43       0.20      0.62      0.30        40\n","          44       0.64      0.68      0.66        40\n","          45       0.12      0.25      0.17        40\n","          46       0.12      0.42      0.18        40\n","          47       0.75      0.23      0.35        40\n","          48       0.00      0.00      0.00        40\n","          49       0.25      0.10      0.14        40\n","          50       0.22      0.28      0.24        40\n","          51       0.16      0.10      0.12        40\n","          52       0.50      0.20      0.29        40\n","          53       0.26      0.38      0.31        40\n","          54       0.10      0.05      0.07        40\n","          55       0.12      0.07      0.09        40\n","          56       0.07      0.30      0.12        40\n","          57       0.26      0.42      0.32        40\n","          58       0.19      0.25      0.22        40\n","          59       0.54      0.55      0.54        40\n","          60       0.00      0.00      0.00        40\n","          61       0.00      0.00      0.00        40\n","          62       0.04      0.12      0.06        40\n","          63       0.00      0.00      0.00        40\n","          64       0.04      0.17      0.07        40\n","          65       0.74      0.42      0.54        40\n","          66       0.50      0.20      0.29        40\n","          67       0.00      0.00      0.00        40\n","          68       0.00      0.00      0.00        40\n","          69       0.12      0.05      0.07        40\n","          70       0.18      0.38      0.24        40\n","          71       0.12      0.12      0.12        40\n","          72       0.38      0.28      0.32        40\n","          73       0.00      0.00      0.00        40\n","          74       0.10      0.10      0.10        40\n","          75       0.00      0.00      0.00        40\n","          76       0.89      0.62      0.74        40\n","          77       0.15      0.07      0.10        40\n","          78       0.04      0.03      0.03        40\n","          79       0.15      0.17      0.16        40\n","          80       0.00      0.00      0.00        40\n","          81       0.00      0.00      0.00        40\n","          82       0.15      0.50      0.23        40\n","          83       0.06      0.10      0.07        40\n","          84       0.33      0.23      0.27        40\n","          85       0.08      0.03      0.04        40\n","          86       0.75      0.23      0.35        40\n","          87       0.89      0.20      0.33        40\n","          88       0.11      0.45      0.18        40\n","          89       0.60      0.07      0.13        40\n","          90       0.14      0.38      0.21        40\n","          91       0.13      0.28      0.17        40\n","          92       0.12      0.07      0.09        40\n","          93       0.00      0.00      0.00        40\n","          94       0.00      0.00      0.00        40\n","\n","    accuracy                           0.14      4400\n","   macro avg       0.18      0.15      0.14      4400\n","weighted avg       0.21      0.14      0.13      4400\n","\n","\n","Confusion Matrix:\n"," [[22  5  3 ...  6  0  4]\n"," [ 0  5  0 ...  0  0  3]\n"," [ 0  0  0 ...  0  0  0]\n"," ...\n"," [ 0  0  0 ...  3  0  2]\n"," [ 0  0  0 ...  0  0  0]\n"," [ 0  0  0 ...  0  0  0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}