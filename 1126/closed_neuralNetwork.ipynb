{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = pd.read_csv('mon.csv')"
      ],
      "metadata": {
        "id": "q0E8WsagOm2Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Networks"
      ],
      "metadata": {
        "id": "E78tZUn1dLKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##데이터 정규화"
      ],
      "metadata": {
        "id": "9eeyc_g8qaFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(columns=['Label'])\n",
        "y = data['Label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "HjXcP2QEa9WF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "정규화 안하면 accuacy 0.03 이렇게 나옴"
      ],
      "metadata": {
        "id": "kLf0WTkrdu0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP"
      ],
      "metadata": {
        "id": "XJTJYcbzqdBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. MLP (Multi-Layer Perceptron)**\n",
        "MLP는 주로 은닉층이 적은 신경망으로, 비교적 간단한 데이터나 구조에서 효율적이며, 과적합을 방지하면서도 좋은 성능을 낼 수 있습니다.\n",
        "\n",
        "- 구조적 간결성: MLP는 일반적으로 1~2개의 은닉층을 사용하기 때문에, 데이터가 복잡하지 않거나 패턴이 명확한 경우 적합합니다. 이 데이터셋이 아주 고차원적이거나 복잡한 구조를 가지지 않았다면, 적은 수의 층과 뉴런만으로도 충분한 성능을 낼 수 있습니다.\n",
        "- 모델 학습의 효율성: MLP는 구조가 단순해 학습 속도가 상대적으로 빠르며, 과적합 방지 기술(예: 정규화)만으로도 안정적인 성능을 낼 수 있습니다. 복잡한 신경망이 필요 없을 때 적은 자원으로 효율적인 성능을 낼 수 있습니다.\n",
        "- 비교적 쉬운 튜닝: DNN에 비해 하이퍼파라미터 튜닝이 비교적 간단하여, 기본 모델로 빠르게 테스트하기에 좋습니다. 예를 들어 은닉층의 개수나 뉴런 수를 큰 조정 없이도 좋은 성능을 기대할 수 있습니다."
      ],
      "metadata": {
        "id": "F_hEFrNja9j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# MLP 모델 정의 및 학습\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
        "mlp_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1_weighted = f1_score(y_test, y_pred, average='weighted')  # weighted 방식으로 F1 점수 계산\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Weighted F1 Score:\", f1_weighted)\n",
        "print(\"MLP 분류 보고서:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMETVHSCbF4P",
        "outputId": "77b4bdb8-8ad0-41d7-b59a-66944082b660"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.714561403508772\n",
            "Weighted F1 Score: 0.7148283868216511\n",
            "MLP 분류 보고서:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.48      0.49        62\n",
            "           1       0.70      0.73      0.72        59\n",
            "           2       0.73      0.93      0.82        55\n",
            "           3       0.54      0.70      0.61        46\n",
            "           4       0.66      0.79      0.72        53\n",
            "           5       0.83      0.78      0.80        63\n",
            "           6       0.73      0.88      0.80        65\n",
            "           7       0.58      0.52      0.55        64\n",
            "           8       0.81      0.58      0.67        59\n",
            "           9       0.58      0.62      0.60        47\n",
            "          10       0.70      0.67      0.68        66\n",
            "          11       0.80      0.72      0.76        57\n",
            "          12       0.85      0.68      0.75        59\n",
            "          13       0.48      0.50      0.49        54\n",
            "          14       0.69      0.66      0.67        58\n",
            "          15       0.58      0.71      0.64        59\n",
            "          16       0.77      0.73      0.75        55\n",
            "          17       0.55      0.67      0.60        67\n",
            "          18       0.85      0.73      0.78        55\n",
            "          19       0.84      0.65      0.73        55\n",
            "          20       0.90      0.96      0.93        68\n",
            "          21       0.83      0.83      0.83        58\n",
            "          22       0.52      0.62      0.57        55\n",
            "          23       0.65      0.67      0.66        58\n",
            "          24       0.58      0.38      0.46        66\n",
            "          25       0.56      0.54      0.55        63\n",
            "          26       0.67      0.74      0.70        54\n",
            "          27       0.78      0.77      0.77        60\n",
            "          28       0.70      0.67      0.69        55\n",
            "          29       0.67      0.77      0.72        64\n",
            "          30       0.78      0.76      0.77        62\n",
            "          31       0.63      0.70      0.67        54\n",
            "          32       0.63      0.62      0.62        65\n",
            "          33       0.78      0.81      0.80        53\n",
            "          34       0.51      0.61      0.55        64\n",
            "          35       0.72      0.89      0.80        56\n",
            "          36       0.85      0.87      0.86        54\n",
            "          37       0.76      0.45      0.57        71\n",
            "          38       0.73      0.69      0.71        70\n",
            "          39       0.78      0.61      0.68        66\n",
            "          40       0.82      0.75      0.78        67\n",
            "          41       0.91      0.78      0.84        63\n",
            "          42       0.70      0.66      0.68        56\n",
            "          43       0.79      0.85      0.82        53\n",
            "          44       0.94      0.94      0.94        62\n",
            "          45       0.43      0.64      0.51        55\n",
            "          46       0.67      0.58      0.62        55\n",
            "          47       0.60      0.69      0.65        59\n",
            "          48       0.88      0.78      0.83        58\n",
            "          49       0.66      0.67      0.67        64\n",
            "          50       0.69      0.88      0.77        50\n",
            "          51       0.60      0.68      0.64        60\n",
            "          52       0.71      0.77      0.74        66\n",
            "          53       0.68      0.80      0.74        49\n",
            "          54       0.76      0.80      0.78        70\n",
            "          55       0.50      0.47      0.48        49\n",
            "          56       0.91      0.94      0.92        63\n",
            "          57       0.68      0.74      0.71        58\n",
            "          58       0.70      0.74      0.72        57\n",
            "          59       0.90      0.86      0.88        64\n",
            "          60       0.85      0.70      0.77        57\n",
            "          61       0.68      0.63      0.66        79\n",
            "          62       0.62      0.80      0.70        75\n",
            "          63       0.45      0.57      0.50        47\n",
            "          64       0.78      0.83      0.80        63\n",
            "          65       0.63      0.64      0.64        53\n",
            "          66       0.96      0.65      0.77        68\n",
            "          67       0.79      0.78      0.78        58\n",
            "          68       0.80      0.79      0.79        70\n",
            "          69       0.63      0.70      0.66        66\n",
            "          70       0.85      0.76      0.80        62\n",
            "          71       0.68      0.75      0.72        57\n",
            "          72       0.95      0.63      0.76        63\n",
            "          73       0.86      0.95      0.90        63\n",
            "          74       0.77      0.66      0.71        73\n",
            "          75       0.85      0.90      0.88        59\n",
            "          76       0.87      0.88      0.88        60\n",
            "          77       0.57      0.57      0.57        51\n",
            "          78       0.58      0.64      0.61        61\n",
            "          79       0.67      0.59      0.63        59\n",
            "          80       0.92      0.86      0.89        66\n",
            "          81       0.71      0.63      0.67        62\n",
            "          82       0.81      0.43      0.57        69\n",
            "          83       0.66      0.85      0.74        53\n",
            "          84       0.80      0.78      0.79        68\n",
            "          85       0.97      0.90      0.93        71\n",
            "          86       0.81      0.81      0.81        53\n",
            "          87       0.73      0.64      0.68        55\n",
            "          88       0.76      0.69      0.72        55\n",
            "          89       0.51      0.65      0.58        55\n",
            "          90       0.67      0.80      0.73        56\n",
            "          91       0.79      0.60      0.68        63\n",
            "          92       0.50      0.64      0.56        55\n",
            "          93       0.96      0.90      0.93        58\n",
            "          94       0.71      0.63      0.67        65\n",
            "\n",
            "    accuracy                           0.71      5700\n",
            "   macro avg       0.72      0.71      0.71      5700\n",
            "weighted avg       0.73      0.71      0.71      5700\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DNN"
      ],
      "metadata": {
        "id": "4dx3ZAoKq9hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. DNN (Deep Neural Network)**\n",
        "DNN은 MLP보다 깊은 은닉층을 여러 개 사용하며, 더 복잡한 데이터나 패턴을 학습할 수 있는 모델입니다.\n",
        "\n",
        "-  복잡한 데이터 패턴 학습: DNN은 다수의 은닉층을 사용하여 데이터 내에 존재할 수 있는 비선형적인 패턴을 잘 학습합니다. 예를 들어, 특성 간의 복잡한 상호작용이나 고차원적인 관계가 있을 때 DNN은 MLP보다 더 우수한 성능을 발휘할 수 있습니다.\n",
        "- 특성 추출력 강화: DNN의 여러 층을 거치면서 데이터에서 **중요한 특징(feature)**들을 점진적으로 추출할 수 있습니다. 이는 데이터의 패턴이 간단하지 않고 복합적일 때, 층이 많아질수록 더 나은 예측 성능을 기대할 수 있다는 점에서 유리합니다.\n",
        "- 과적합 방지 및 정규화 기법 활용: DNN은 드롭아웃(dropout), 배치 정규화(batch normalization) 등의 다양한 정규화 기법을 적용할 수 있어, 모델이 깊어질수록 발생할 수 있는 과적합 문제를 줄일 수 있습니다.\n",
        "확장 가능성: DNN은 더 깊은 층과 다양한 뉴런 수로 확장할 수 있어, MLP로 충분한 성능을 얻지 못할 때 모델의 성능을 지속적으로 향상할 수 있는 여지가 많습니다."
      ],
      "metadata": {
        "id": "M_LBWRh9bRTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report"
      ],
      "metadata": {
        "id": "P5CNfNYBd3ik"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 2. DNN 모델 학습 ###\n",
        "\n",
        "#레이블 원-핫 인코딩 (Keras에서 필요)\n",
        "num_classes = len(y.unique())\n",
        "y_train_categorical = to_categorical(y_train, num_classes)\n",
        "y_test_categorical = to_categorical(y_test, num_classes)\n",
        "\n",
        "# DNN 모델 정의\n",
        "dnn_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(num_classes, activation='softmax')  # 다중 클래스 분류\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "dnn_model.fit(X_train, y_train_categorical, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# 모델 평가\n",
        "y_pred_proba = dnn_model.predict(X_test)  # 클래스 확률 예측\n",
        "y_pred = y_pred_proba.argmax(axis=1)  # 확률이 가장 높은 클래스 선택\n",
        "\n",
        "# 성능 평가\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Weighted F1 Score:\", f1_weighted)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4BcDugEbhVn",
        "outputId": "0096d777-ebf6-4d0d-b74c-13ffa77dfd2e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.0574 - loss: 4.3958 - val_accuracy: 0.2090 - val_loss: 3.5559\n",
            "Epoch 2/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.1755 - loss: 3.4769 - val_accuracy: 0.2944 - val_loss: 3.0306\n",
            "Epoch 3/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.2203 - loss: 3.2087 - val_accuracy: 0.3538 - val_loss: 2.8047\n",
            "Epoch 4/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2732 - loss: 3.0163 - val_accuracy: 0.3850 - val_loss: 2.6206\n",
            "Epoch 5/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2840 - loss: 2.9048 - val_accuracy: 0.4086 - val_loss: 2.4889\n",
            "Epoch 6/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3151 - loss: 2.7531 - val_accuracy: 0.4346 - val_loss: 2.3543\n",
            "Epoch 7/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3240 - loss: 2.7103 - val_accuracy: 0.4398 - val_loss: 2.3167\n",
            "Epoch 8/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3380 - loss: 2.6444 - val_accuracy: 0.4774 - val_loss: 2.2101\n",
            "Epoch 9/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3511 - loss: 2.5709 - val_accuracy: 0.4835 - val_loss: 2.1662\n",
            "Epoch 10/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3658 - loss: 2.4972 - val_accuracy: 0.4925 - val_loss: 2.0962\n",
            "Epoch 11/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3709 - loss: 2.4510 - val_accuracy: 0.4944 - val_loss: 2.0717\n",
            "Epoch 12/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3732 - loss: 2.4452 - val_accuracy: 0.5147 - val_loss: 2.0119\n",
            "Epoch 13/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3926 - loss: 2.3737 - val_accuracy: 0.5248 - val_loss: 1.9680\n",
            "Epoch 14/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3990 - loss: 2.3500 - val_accuracy: 0.5286 - val_loss: 1.9368\n",
            "Epoch 15/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3976 - loss: 2.3610 - val_accuracy: 0.5338 - val_loss: 1.9150\n",
            "Epoch 16/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4083 - loss: 2.3129 - val_accuracy: 0.5391 - val_loss: 1.8993\n",
            "Epoch 17/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4103 - loss: 2.2679 - val_accuracy: 0.5350 - val_loss: 1.8635\n",
            "Epoch 18/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4100 - loss: 2.2634 - val_accuracy: 0.5436 - val_loss: 1.8393\n",
            "Epoch 19/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4158 - loss: 2.2359 - val_accuracy: 0.5511 - val_loss: 1.8267\n",
            "Epoch 20/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4141 - loss: 2.2238 - val_accuracy: 0.5504 - val_loss: 1.8119\n",
            "Epoch 21/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4292 - loss: 2.1824 - val_accuracy: 0.5579 - val_loss: 1.7725\n",
            "Epoch 22/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4177 - loss: 2.2100 - val_accuracy: 0.5598 - val_loss: 1.7734\n",
            "Epoch 23/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4456 - loss: 2.1573 - val_accuracy: 0.5662 - val_loss: 1.7485\n",
            "Epoch 24/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4206 - loss: 2.1817 - val_accuracy: 0.5722 - val_loss: 1.7475\n",
            "Epoch 25/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4353 - loss: 2.1441 - val_accuracy: 0.5658 - val_loss: 1.7301\n",
            "Epoch 26/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4464 - loss: 2.1230 - val_accuracy: 0.5647 - val_loss: 1.7387\n",
            "Epoch 27/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4527 - loss: 2.1154 - val_accuracy: 0.5662 - val_loss: 1.7072\n",
            "Epoch 28/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4369 - loss: 2.1209 - val_accuracy: 0.5850 - val_loss: 1.6943\n",
            "Epoch 29/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4512 - loss: 2.0992 - val_accuracy: 0.5846 - val_loss: 1.6844\n",
            "Epoch 30/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4499 - loss: 2.0870 - val_accuracy: 0.5857 - val_loss: 1.6657\n",
            "Epoch 31/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4420 - loss: 2.1263 - val_accuracy: 0.5831 - val_loss: 1.6693\n",
            "Epoch 32/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4606 - loss: 2.0655 - val_accuracy: 0.5925 - val_loss: 1.6474\n",
            "Epoch 33/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4478 - loss: 2.0615 - val_accuracy: 0.5846 - val_loss: 1.6450\n",
            "Epoch 34/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4412 - loss: 2.0842 - val_accuracy: 0.5947 - val_loss: 1.6370\n",
            "Epoch 35/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4524 - loss: 2.0587 - val_accuracy: 0.5906 - val_loss: 1.6210\n",
            "Epoch 36/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4518 - loss: 2.0533 - val_accuracy: 0.5895 - val_loss: 1.6412\n",
            "Epoch 37/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4536 - loss: 2.0551 - val_accuracy: 0.5861 - val_loss: 1.6244\n",
            "Epoch 38/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4523 - loss: 2.0486 - val_accuracy: 0.5951 - val_loss: 1.6132\n",
            "Epoch 39/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4597 - loss: 2.0232 - val_accuracy: 0.5831 - val_loss: 1.6384\n",
            "Epoch 40/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4643 - loss: 2.0211 - val_accuracy: 0.5992 - val_loss: 1.5849\n",
            "Epoch 41/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4644 - loss: 2.0234 - val_accuracy: 0.6019 - val_loss: 1.5819\n",
            "Epoch 42/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4630 - loss: 2.0122 - val_accuracy: 0.5992 - val_loss: 1.5960\n",
            "Epoch 43/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4639 - loss: 2.0090 - val_accuracy: 0.6023 - val_loss: 1.5803\n",
            "Epoch 44/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4585 - loss: 2.0164 - val_accuracy: 0.6060 - val_loss: 1.5883\n",
            "Epoch 45/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4657 - loss: 1.9760 - val_accuracy: 0.6034 - val_loss: 1.5819\n",
            "Epoch 46/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4702 - loss: 2.0155 - val_accuracy: 0.6008 - val_loss: 1.5762\n",
            "Epoch 47/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4727 - loss: 1.9819 - val_accuracy: 0.6083 - val_loss: 1.5627\n",
            "Epoch 48/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4571 - loss: 2.0488 - val_accuracy: 0.6045 - val_loss: 1.5416\n",
            "Epoch 49/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4697 - loss: 1.9984 - val_accuracy: 0.6079 - val_loss: 1.5532\n",
            "Epoch 50/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4794 - loss: 1.9828 - val_accuracy: 0.6165 - val_loss: 1.5378\n",
            "Epoch 51/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4735 - loss: 1.9922 - val_accuracy: 0.6147 - val_loss: 1.5596\n",
            "Epoch 52/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4770 - loss: 1.9771 - val_accuracy: 0.6102 - val_loss: 1.5478\n",
            "Epoch 53/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4764 - loss: 1.9690 - val_accuracy: 0.6135 - val_loss: 1.5413\n",
            "Epoch 54/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4681 - loss: 1.9636 - val_accuracy: 0.6180 - val_loss: 1.5206\n",
            "Epoch 55/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4862 - loss: 1.9611 - val_accuracy: 0.6105 - val_loss: 1.5331\n",
            "Epoch 56/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4730 - loss: 1.9417 - val_accuracy: 0.6192 - val_loss: 1.5205\n",
            "Epoch 57/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4847 - loss: 1.9181 - val_accuracy: 0.6105 - val_loss: 1.5286\n",
            "Epoch 58/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4782 - loss: 1.9747 - val_accuracy: 0.6199 - val_loss: 1.5127\n",
            "Epoch 59/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4865 - loss: 1.9610 - val_accuracy: 0.6169 - val_loss: 1.5171\n",
            "Epoch 60/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4777 - loss: 1.9596 - val_accuracy: 0.6259 - val_loss: 1.5011\n",
            "Epoch 61/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4827 - loss: 1.9188 - val_accuracy: 0.6132 - val_loss: 1.5211\n",
            "Epoch 62/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4733 - loss: 1.9379 - val_accuracy: 0.6147 - val_loss: 1.5043\n",
            "Epoch 63/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4841 - loss: 1.9470 - val_accuracy: 0.6117 - val_loss: 1.5112\n",
            "Epoch 64/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4784 - loss: 1.9641 - val_accuracy: 0.6143 - val_loss: 1.4992\n",
            "Epoch 65/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4836 - loss: 1.9262 - val_accuracy: 0.6195 - val_loss: 1.5014\n",
            "Epoch 66/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4851 - loss: 1.8926 - val_accuracy: 0.6165 - val_loss: 1.4996\n",
            "Epoch 67/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4812 - loss: 1.9557 - val_accuracy: 0.6192 - val_loss: 1.4950\n",
            "Epoch 68/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4766 - loss: 1.9396 - val_accuracy: 0.6173 - val_loss: 1.4894\n",
            "Epoch 69/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4838 - loss: 1.9155 - val_accuracy: 0.6248 - val_loss: 1.5009\n",
            "Epoch 70/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4926 - loss: 1.9208 - val_accuracy: 0.6169 - val_loss: 1.4932\n",
            "Epoch 71/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4768 - loss: 1.9539 - val_accuracy: 0.6256 - val_loss: 1.4980\n",
            "Epoch 72/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4882 - loss: 1.9158 - val_accuracy: 0.6226 - val_loss: 1.4918\n",
            "Epoch 73/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4793 - loss: 1.9258 - val_accuracy: 0.6271 - val_loss: 1.4896\n",
            "Epoch 74/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.4934 - loss: 1.8919 - val_accuracy: 0.6252 - val_loss: 1.4913\n",
            "Epoch 75/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.4819 - loss: 1.9266 - val_accuracy: 0.6327 - val_loss: 1.4778\n",
            "Epoch 76/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4849 - loss: 1.9244 - val_accuracy: 0.6297 - val_loss: 1.4711\n",
            "Epoch 77/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4934 - loss: 1.9485 - val_accuracy: 0.6271 - val_loss: 1.4698\n",
            "Epoch 78/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4895 - loss: 1.9414 - val_accuracy: 0.6308 - val_loss: 1.4687\n",
            "Epoch 79/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4887 - loss: 1.8917 - val_accuracy: 0.6263 - val_loss: 1.4678\n",
            "Epoch 80/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4938 - loss: 1.8918 - val_accuracy: 0.6244 - val_loss: 1.4959\n",
            "Epoch 81/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4964 - loss: 1.8981 - val_accuracy: 0.6233 - val_loss: 1.4657\n",
            "Epoch 82/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4949 - loss: 1.9051 - val_accuracy: 0.6297 - val_loss: 1.4650\n",
            "Epoch 83/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5018 - loss: 1.8638 - val_accuracy: 0.6346 - val_loss: 1.4527\n",
            "Epoch 84/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4952 - loss: 1.9098 - val_accuracy: 0.6293 - val_loss: 1.4755\n",
            "Epoch 85/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4959 - loss: 1.8843 - val_accuracy: 0.6395 - val_loss: 1.4528\n",
            "Epoch 86/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4889 - loss: 1.8928 - val_accuracy: 0.6289 - val_loss: 1.4543\n",
            "Epoch 87/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4838 - loss: 1.9257 - val_accuracy: 0.6402 - val_loss: 1.4593\n",
            "Epoch 88/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4873 - loss: 1.9132 - val_accuracy: 0.6248 - val_loss: 1.4528\n",
            "Epoch 89/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4880 - loss: 1.9027 - val_accuracy: 0.6414 - val_loss: 1.4414\n",
            "Epoch 90/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4926 - loss: 1.9074 - val_accuracy: 0.6406 - val_loss: 1.4518\n",
            "Epoch 91/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5061 - loss: 1.8745 - val_accuracy: 0.6365 - val_loss: 1.4582\n",
            "Epoch 92/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4984 - loss: 1.8904 - val_accuracy: 0.6301 - val_loss: 1.4428\n",
            "Epoch 93/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4820 - loss: 1.9196 - val_accuracy: 0.6387 - val_loss: 1.4380\n",
            "Epoch 94/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4990 - loss: 1.8533 - val_accuracy: 0.6432 - val_loss: 1.4413\n",
            "Epoch 95/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5032 - loss: 1.8646 - val_accuracy: 0.6353 - val_loss: 1.4373\n",
            "Epoch 96/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5078 - loss: 1.8749 - val_accuracy: 0.6380 - val_loss: 1.4326\n",
            "Epoch 97/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4989 - loss: 1.8399 - val_accuracy: 0.6380 - val_loss: 1.4306\n",
            "Epoch 98/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4898 - loss: 1.8611 - val_accuracy: 0.6316 - val_loss: 1.4269\n",
            "Epoch 99/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4939 - loss: 1.8690 - val_accuracy: 0.6350 - val_loss: 1.4483\n",
            "Epoch 100/100\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5088 - loss: 1.8367 - val_accuracy: 0.6372 - val_loss: 1.4314\n",
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Accuracy: 0.613859649122807\n",
            "Weighted F1 Score: 0.6057753784999669\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.24      0.37        62\n",
            "           1       0.86      0.71      0.78        59\n",
            "           2       0.61      0.64      0.62        55\n",
            "           3       0.60      0.61      0.60        46\n",
            "           4       0.60      0.60      0.60        53\n",
            "           5       0.89      0.65      0.75        63\n",
            "           6       0.61      0.85      0.71        65\n",
            "           7       0.75      0.52      0.61        64\n",
            "           8       0.59      0.32      0.42        59\n",
            "           9       0.53      0.38      0.44        47\n",
            "          10       0.73      0.62      0.67        66\n",
            "          11       0.72      0.63      0.67        57\n",
            "          12       0.72      0.73      0.72        59\n",
            "          13       0.50      0.22      0.31        54\n",
            "          14       0.48      0.53      0.51        58\n",
            "          15       0.43      0.56      0.49        59\n",
            "          16       0.68      0.58      0.63        55\n",
            "          17       0.50      0.52      0.51        67\n",
            "          18       0.81      0.85      0.83        55\n",
            "          19       0.63      0.73      0.68        55\n",
            "          20       0.82      0.91      0.86        68\n",
            "          21       0.66      0.83      0.73        58\n",
            "          22       0.37      0.49      0.42        55\n",
            "          23       0.71      0.62      0.66        58\n",
            "          24       0.38      0.45      0.41        66\n",
            "          25       0.64      0.11      0.19        63\n",
            "          26       0.42      0.59      0.49        54\n",
            "          27       0.78      0.78      0.78        60\n",
            "          28       0.70      0.42      0.52        55\n",
            "          29       0.55      0.72      0.62        64\n",
            "          30       0.57      0.66      0.61        62\n",
            "          31       0.38      0.61      0.47        54\n",
            "          32       0.84      0.40      0.54        65\n",
            "          33       0.55      0.91      0.69        53\n",
            "          34       0.51      0.39      0.44        64\n",
            "          35       0.77      0.86      0.81        56\n",
            "          36       0.81      0.78      0.79        54\n",
            "          37       0.71      0.28      0.40        71\n",
            "          38       0.64      0.53      0.58        70\n",
            "          39       0.81      0.67      0.73        66\n",
            "          40       0.73      0.49      0.59        67\n",
            "          41       0.59      0.75      0.66        63\n",
            "          42       0.56      0.61      0.58        56\n",
            "          43       0.79      0.77      0.78        53\n",
            "          44       0.67      0.97      0.79        62\n",
            "          45       0.33      0.44      0.38        55\n",
            "          46       0.51      0.56      0.53        55\n",
            "          47       0.41      0.51      0.45        59\n",
            "          48       0.62      0.69      0.66        58\n",
            "          49       0.59      0.31      0.41        64\n",
            "          50       0.75      0.72      0.73        50\n",
            "          51       0.46      0.67      0.54        60\n",
            "          52       0.79      0.33      0.47        66\n",
            "          53       0.50      0.67      0.57        49\n",
            "          54       0.66      0.54      0.59        70\n",
            "          55       0.29      0.27      0.28        49\n",
            "          56       0.81      0.87      0.84        63\n",
            "          57       0.67      0.59      0.62        58\n",
            "          58       0.49      0.84      0.62        57\n",
            "          59       0.72      0.86      0.79        64\n",
            "          60       0.77      0.65      0.70        57\n",
            "          61       0.88      0.35      0.50        79\n",
            "          62       0.60      0.45      0.52        75\n",
            "          63       0.38      0.47      0.42        47\n",
            "          64       0.73      0.63      0.68        63\n",
            "          65       0.44      0.58      0.50        53\n",
            "          66       0.74      0.76      0.75        68\n",
            "          67       0.69      0.64      0.66        58\n",
            "          68       0.87      0.74      0.80        70\n",
            "          69       0.64      0.65      0.65        66\n",
            "          70       0.63      0.90      0.74        62\n",
            "          71       0.52      0.54      0.53        57\n",
            "          72       0.74      0.49      0.59        63\n",
            "          73       0.65      0.95      0.77        63\n",
            "          74       0.56      0.67      0.61        73\n",
            "          75       0.76      0.88      0.82        59\n",
            "          76       0.93      0.87      0.90        60\n",
            "          77       0.34      0.24      0.28        51\n",
            "          78       0.44      0.64      0.52        61\n",
            "          79       0.64      0.59      0.61        59\n",
            "          80       0.75      0.77      0.76        66\n",
            "          81       0.66      0.37      0.47        62\n",
            "          82       0.63      0.46      0.53        69\n",
            "          83       0.41      0.58      0.48        53\n",
            "          84       0.63      0.81      0.71        68\n",
            "          85       0.83      0.85      0.84        71\n",
            "          86       0.69      0.87      0.77        53\n",
            "          87       0.55      0.71      0.62        55\n",
            "          88       0.58      0.69      0.63        55\n",
            "          89       0.40      0.55      0.46        55\n",
            "          90       0.68      0.70      0.69        56\n",
            "          91       0.63      0.49      0.55        63\n",
            "          92       0.41      0.22      0.29        55\n",
            "          93       0.74      0.91      0.82        58\n",
            "          94       0.48      0.65      0.55        65\n",
            "\n",
            "    accuracy                           0.61      5700\n",
            "   macro avg       0.63      0.61      0.60      5700\n",
            "weighted avg       0.63      0.61      0.61      5700\n",
            "\n"
          ]
        }
      ]
    }
  ]
}