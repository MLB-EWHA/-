{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSVvsLvgo3s3","executionInfo":{"status":"ok","timestamp":1732544040839,"user_tz":-540,"elapsed":24112,"user":{"displayName":"오윤재","userId":"04375251631092764589"}},"outputId":"7a68884c-cf48-4688-ce0c-8acb22adbce5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# 라이브러리 임포트\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# 데이터 로드\n","data = pd.read_csv(\"/content/drive/MyDrive/monunmon.csv\")\n","\n","# 특징(X)과 레이블(y) 분리\n","X = data.iloc[:, :-1].values  # 특징 (feature)들\n","y = data.iloc[:, -1].values   # 레이블 (0~94)\n","\n","# 데이터 정규화\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# 학습 데이터와 테스트 데이터 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 레이블 원-핫 인코딩 (One-hot encoding)\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes=95)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes=95)\n","\n","# Neural Network 모델 생성\n","model = Sequential()\n","model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # 입력층\n","model.add(Dropout(0.3))\n","model.add(Dense(64, activation='relu'))  # 은닉층\n","model.add(Dropout(0.3))\n","model.add(Dense(95, activation='softmax'))  # 출력층 (0~94 레이블, 총 95개)\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 학습\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n","\n","# 모델 평가\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n","\n","# 테스트 데이터 예측\n","y_test_pred = model.predict(X_test)\n","y_test_pred_classes = np.argmax(y_test_pred, axis=1)  # 예측 확률에서 클래스 추출\n","y_test_true_classes = np.argmax(y_test, axis=1)  # 원-핫 인코딩 된 실제 값을 클래스 형태로 변환\n","\n","# F1 Score 계산\n","from sklearn.metrics import f1_score\n","f1 = f1_score(y_test_true_classes, y_test_pred_classes, average='weighted')\n","print(f\"F1 Score (Weighted): {f1:.4f}\")\n","\n","# Classification Report 출력\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test_true_classes, y_test_pred_classes))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"asO0nQudwuk9","executionInfo":{"status":"ok","timestamp":1732544593924,"user_tz":-540,"elapsed":94694,"user":{"displayName":"오윤재","userId":"04375251631092764589"}},"outputId":"65d54bf5-5c4c-4138-eacc-ab3e7c7628aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.1376 - loss: 4.0605 - val_accuracy: 0.1973 - val_loss: 3.2745\n","Epoch 2/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1811 - loss: 3.3600 - val_accuracy: 0.2505 - val_loss: 3.0058\n","Epoch 3/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2181 - loss: 3.1406 - val_accuracy: 0.2900 - val_loss: 2.8284\n","Epoch 4/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2475 - loss: 3.0063 - val_accuracy: 0.3034 - val_loss: 2.7133\n","Epoch 5/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2712 - loss: 2.8954 - val_accuracy: 0.3400 - val_loss: 2.6086\n","Epoch 6/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2828 - loss: 2.8016 - val_accuracy: 0.3532 - val_loss: 2.5373\n","Epoch 7/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2978 - loss: 2.7405 - val_accuracy: 0.3648 - val_loss: 2.4720\n","Epoch 8/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3118 - loss: 2.6695 - val_accuracy: 0.3795 - val_loss: 2.4019\n","Epoch 9/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3228 - loss: 2.6057 - val_accuracy: 0.3861 - val_loss: 2.3644\n","Epoch 10/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.3244 - loss: 2.5813 - val_accuracy: 0.3982 - val_loss: 2.3154\n","Epoch 11/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3363 - loss: 2.5491 - val_accuracy: 0.4132 - val_loss: 2.2730\n","Epoch 12/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.3430 - loss: 2.5254 - val_accuracy: 0.4191 - val_loss: 2.2464\n","Epoch 13/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.3477 - loss: 2.4673 - val_accuracy: 0.4200 - val_loss: 2.2092\n","Epoch 14/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3535 - loss: 2.4523 - val_accuracy: 0.4211 - val_loss: 2.1845\n","Epoch 15/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3621 - loss: 2.4133 - val_accuracy: 0.4448 - val_loss: 2.1486\n","Epoch 16/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.3588 - loss: 2.4085 - val_accuracy: 0.4411 - val_loss: 2.1336\n","Epoch 17/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3605 - loss: 2.3951 - val_accuracy: 0.4475 - val_loss: 2.0939\n","Epoch 18/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3670 - loss: 2.3663 - val_accuracy: 0.4514 - val_loss: 2.0880\n","Epoch 19/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3788 - loss: 2.3491 - val_accuracy: 0.4548 - val_loss: 2.0608\n","Epoch 20/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.3764 - loss: 2.3070 - val_accuracy: 0.4636 - val_loss: 2.0401\n","Epoch 21/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3846 - loss: 2.3124 - val_accuracy: 0.4730 - val_loss: 2.0151\n","Epoch 22/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3853 - loss: 2.2938 - val_accuracy: 0.4761 - val_loss: 1.9998\n","Epoch 23/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3893 - loss: 2.2835 - val_accuracy: 0.4789 - val_loss: 1.9888\n","Epoch 24/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3896 - loss: 2.2694 - val_accuracy: 0.4820 - val_loss: 1.9684\n","Epoch 25/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3921 - loss: 2.2687 - val_accuracy: 0.4827 - val_loss: 1.9636\n","Epoch 26/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4000 - loss: 2.2275 - val_accuracy: 0.4814 - val_loss: 1.9436\n","Epoch 27/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3915 - loss: 2.2461 - val_accuracy: 0.4905 - val_loss: 1.9225\n","Epoch 28/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3980 - loss: 2.2375 - val_accuracy: 0.4939 - val_loss: 1.9087\n","Epoch 29/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4100 - loss: 2.1922 - val_accuracy: 0.5030 - val_loss: 1.8935\n","Epoch 30/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4142 - loss: 2.1746 - val_accuracy: 0.5050 - val_loss: 1.8918\n","Epoch 31/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4119 - loss: 2.1770 - val_accuracy: 0.5084 - val_loss: 1.8832\n","Epoch 32/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4116 - loss: 2.1807 - val_accuracy: 0.4966 - val_loss: 1.8702\n","Epoch 33/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4178 - loss: 2.1441 - val_accuracy: 0.5125 - val_loss: 1.8502\n","Epoch 34/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4158 - loss: 2.1521 - val_accuracy: 0.5070 - val_loss: 1.8505\n","Epoch 35/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4133 - loss: 2.1541 - val_accuracy: 0.5105 - val_loss: 1.8366\n","Epoch 36/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4192 - loss: 2.1470 - val_accuracy: 0.5207 - val_loss: 1.8304\n","Epoch 37/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4224 - loss: 2.1142 - val_accuracy: 0.5245 - val_loss: 1.8185\n","Epoch 38/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4340 - loss: 2.1185 - val_accuracy: 0.5295 - val_loss: 1.8094\n","Epoch 39/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4289 - loss: 2.1118 - val_accuracy: 0.5225 - val_loss: 1.7989\n","Epoch 40/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4400 - loss: 2.0987 - val_accuracy: 0.5277 - val_loss: 1.7891\n","Epoch 41/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4345 - loss: 2.0822 - val_accuracy: 0.5302 - val_loss: 1.7802\n","Epoch 42/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4349 - loss: 2.0823 - val_accuracy: 0.5200 - val_loss: 1.7824\n","Epoch 43/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4316 - loss: 2.0870 - val_accuracy: 0.5302 - val_loss: 1.7670\n","Epoch 44/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4366 - loss: 2.0627 - val_accuracy: 0.5361 - val_loss: 1.7566\n","Epoch 45/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.4241 - loss: 2.0901 - val_accuracy: 0.5361 - val_loss: 1.7564\n","Epoch 46/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4363 - loss: 2.0385 - val_accuracy: 0.5416 - val_loss: 1.7512\n","Epoch 47/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4345 - loss: 2.0613 - val_accuracy: 0.5395 - val_loss: 1.7464\n","Epoch 48/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4454 - loss: 2.0274 - val_accuracy: 0.5345 - val_loss: 1.7406\n","Epoch 49/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4390 - loss: 2.0506 - val_accuracy: 0.5445 - val_loss: 1.7171\n","Epoch 50/50\n","\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4497 - loss: 2.0246 - val_accuracy: 0.5407 - val_loss: 1.7277\n","\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5623 - loss: 1.6937\n","\n","Test Accuracy: 0.5407\n","\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","F1 Score (Weighted): 0.5234\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.23      0.36        30\n","           1       0.65      0.45      0.53        44\n","           2       0.55      0.56      0.55        41\n","           3       0.61      0.33      0.43        33\n","           4       0.46      0.59      0.52        32\n","           5       0.67      0.43      0.52        37\n","           6       0.40      0.76      0.52        38\n","           7       0.53      0.23      0.32        35\n","           8       0.57      0.24      0.34        33\n","           9       0.50      0.38      0.43        26\n","          10       0.73      0.44      0.55        43\n","          11       0.71      0.49      0.58        45\n","          12       0.62      0.68      0.65        44\n","          13       0.67      0.09      0.15        46\n","          14       0.71      0.41      0.52        37\n","          15       0.44      0.36      0.39        39\n","          16       0.87      0.53      0.66        51\n","          17       0.57      0.30      0.40        53\n","          18       0.89      0.82      0.85        38\n","          19       0.52      0.68      0.59        38\n","          20       0.86      0.88      0.87        48\n","          21       0.59      0.50      0.54        34\n","          22       0.53      0.41      0.46        51\n","          23       0.73      0.41      0.52        39\n","          24       0.40      0.32      0.35        38\n","          25       0.40      0.05      0.09        38\n","          26       0.56      0.51      0.54        37\n","          27       0.71      0.74      0.73        47\n","          28       0.79      0.48      0.60        31\n","          29       0.57      0.79      0.66        43\n","          30       0.53      0.69      0.60        39\n","          31       0.68      0.30      0.41        44\n","          32       0.69      0.50      0.58        48\n","          33       0.75      0.69      0.72        39\n","          34       0.43      0.42      0.42        36\n","          35       0.69      0.75      0.72        32\n","          36       0.81      0.69      0.74        42\n","          37       0.43      0.07      0.12        42\n","          38       0.45      0.65      0.53        49\n","          39       0.53      0.70      0.60        43\n","          40       0.58      0.24      0.34        45\n","          41       0.69      0.78      0.73        46\n","          42       0.58      0.67      0.62        43\n","          43       0.76      0.45      0.56        49\n","          44       0.48      0.95      0.63        42\n","          45       0.36      0.20      0.26        45\n","          46       0.59      0.32      0.42        31\n","          47       0.62      0.50      0.55        48\n","          48       0.76      0.57      0.65        44\n","          49       0.71      0.44      0.55        34\n","          50       0.57      0.66      0.61        32\n","          51       0.44      0.44      0.44        48\n","          52       0.76      0.23      0.35        57\n","          53       0.40      0.29      0.34        41\n","          54       0.54      0.42      0.47        36\n","          55       0.42      0.11      0.18        44\n","          56       0.54      0.83      0.65        41\n","          57       0.63      0.59      0.61        32\n","          58       0.60      0.79      0.68        42\n","          59       0.58      0.81      0.67        36\n","          60       0.75      0.60      0.67        35\n","          61       0.71      0.27      0.39        37\n","          62       0.55      0.28      0.37        43\n","          63       0.53      0.47      0.49        43\n","          64       0.60      0.71      0.65        35\n","          65       0.62      0.54      0.58        39\n","          66       0.44      0.63      0.52        35\n","          67       0.69      0.69      0.69        42\n","          68       0.50      0.23      0.31        40\n","          69       0.41      0.57      0.48        42\n","          70       0.55      0.93      0.69        41\n","          71       0.24      0.45      0.31        29\n","          72       1.00      0.46      0.63        56\n","          73       0.53      0.93      0.68        43\n","          74       0.35      0.44      0.39        32\n","          75       0.77      0.93      0.84        40\n","          76       0.79      0.81      0.80        37\n","          77       0.25      0.19      0.22        31\n","          78       0.38      0.67      0.48        33\n","          79       0.65      0.45      0.53        53\n","          80       0.63      0.85      0.72        40\n","          81       0.63      0.31      0.41        39\n","          82       0.45      0.42      0.44        40\n","          83       0.37      0.46      0.41        39\n","          84       0.62      0.72      0.67        36\n","          85       0.76      0.79      0.77        47\n","          86       0.52      0.77      0.62        35\n","          87       0.65      0.41      0.50        37\n","          88       0.52      0.44      0.48        27\n","          89       0.39      0.35      0.37        40\n","          90       0.63      0.53      0.57        51\n","          91       0.35      0.15      0.21        40\n","          92       0.00      0.00      0.00        29\n","          93       0.79      0.96      0.86        50\n","          94       0.42      0.68      0.52       625\n","\n","    accuracy                           0.54      4400\n","   macro avg       0.58      0.52      0.52      4400\n","weighted avg       0.57      0.54      0.52      4400\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]}]}