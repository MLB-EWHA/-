{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSVvsLvgo3s3","executionInfo":{"status":"ok","timestamp":1733061378162,"user_tz":-540,"elapsed":27344,"user":{"displayName":"오윤재","userId":"04375251631092764589"}},"outputId":"6e2a1704-34a1-4016-c5d3-07248cd03c5d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# 라이브러리 임포트\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# 데이터 로드\n","data = pd.read_csv(\"/content/drive/MyDrive/mon.csv\")\n","\n","# 특징(X)과 레이블(y) 분리\n","X = data.iloc[:, :-1].values  # 특징 (feature)들\n","y = data.iloc[:, -1].values   # 레이블 (0~94)\n","\n","# 데이터 정규화\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# 학습 데이터와 테스트 데이터 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 레이블 원-핫 인코딩 (One-hot encoding)\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes=95)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes=95)\n","\n","# Neural Network 모델 생성\n","model = Sequential()\n","model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # 입력층\n","model.add(Dropout(0.3))\n","model.add(Dense(64, activation='relu'))  # 은닉층\n","model.add(Dropout(0.3))\n","model.add(Dense(95, activation='softmax'))  # 출력층 (0~94 레이블, 총 95개)\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 학습\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n","\n","# 모델 평가\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n","\n","# 테스트 데이터 예측\n","y_test_pred = model.predict(X_test)\n","y_test_pred_classes = np.argmax(y_test_pred, axis=1)  # 예측 확률에서 클래스 추출\n","y_test_true_classes = np.argmax(y_test, axis=1)  # 원-핫 인코딩 된 실제 값을 클래스 형태로 변환\n","\n","# F1 Score 계산\n","from sklearn.metrics import f1_score\n","f1 = f1_score(y_test_true_classes, y_test_pred_classes, average='weighted')\n","print(f\"F1 Score (Weighted): {f1:.4f}\")\n","\n","# Classification Report 출력\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test_true_classes, y_test_pred_classes))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"asO0nQudwuk9","executionInfo":{"status":"ok","timestamp":1733061461988,"user_tz":-540,"elapsed":75518,"user":{"displayName":"오윤재","userId":"04375251631092764589"}},"outputId":"90034638-1df7-4ed4-a9db-8e58aa62deb3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0450 - loss: 4.2732 - val_accuracy: 0.1653 - val_loss: 3.4566\n","Epoch 2/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1375 - loss: 3.5355 - val_accuracy: 0.2205 - val_loss: 3.1904\n","Epoch 3/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1758 - loss: 3.3122 - val_accuracy: 0.2705 - val_loss: 3.0124\n","Epoch 4/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2148 - loss: 3.1643 - val_accuracy: 0.2992 - val_loss: 2.8630\n","Epoch 5/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2438 - loss: 3.0254 - val_accuracy: 0.3292 - val_loss: 2.7360\n","Epoch 6/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2587 - loss: 2.9382 - val_accuracy: 0.3466 - val_loss: 2.6371\n","Epoch 7/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2853 - loss: 2.8348 - val_accuracy: 0.3634 - val_loss: 2.5698\n","Epoch 8/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3027 - loss: 2.7616 - val_accuracy: 0.3792 - val_loss: 2.5068\n","Epoch 9/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3132 - loss: 2.7041 - val_accuracy: 0.3832 - val_loss: 2.4535\n","Epoch 10/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3146 - loss: 2.6723 - val_accuracy: 0.3955 - val_loss: 2.3932\n","Epoch 11/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3229 - loss: 2.6438 - val_accuracy: 0.3997 - val_loss: 2.3568\n","Epoch 12/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3338 - loss: 2.5805 - val_accuracy: 0.4124 - val_loss: 2.3079\n","Epoch 13/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3378 - loss: 2.5623 - val_accuracy: 0.4129 - val_loss: 2.2877\n","Epoch 14/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3411 - loss: 2.5196 - val_accuracy: 0.4266 - val_loss: 2.2532\n","Epoch 15/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3530 - loss: 2.5054 - val_accuracy: 0.4358 - val_loss: 2.2157\n","Epoch 16/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.3635 - loss: 2.4657 - val_accuracy: 0.4429 - val_loss: 2.1797\n","Epoch 17/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3723 - loss: 2.4301 - val_accuracy: 0.4526 - val_loss: 2.1427\n","Epoch 18/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3715 - loss: 2.4176 - val_accuracy: 0.4463 - val_loss: 2.1164\n","Epoch 19/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3837 - loss: 2.3683 - val_accuracy: 0.4592 - val_loss: 2.1011\n","Epoch 20/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3831 - loss: 2.3528 - val_accuracy: 0.4682 - val_loss: 2.0677\n","Epoch 21/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3823 - loss: 2.3635 - val_accuracy: 0.4821 - val_loss: 2.0447\n","Epoch 22/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3902 - loss: 2.3358 - val_accuracy: 0.4803 - val_loss: 2.0337\n","Epoch 23/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3859 - loss: 2.3260 - val_accuracy: 0.4903 - val_loss: 2.0140\n","Epoch 24/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3996 - loss: 2.2922 - val_accuracy: 0.4961 - val_loss: 1.9946\n","Epoch 25/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4051 - loss: 2.2649 - val_accuracy: 0.4947 - val_loss: 1.9775\n","Epoch 26/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4174 - loss: 2.2457 - val_accuracy: 0.4924 - val_loss: 1.9723\n","Epoch 27/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4155 - loss: 2.2346 - val_accuracy: 0.5026 - val_loss: 1.9353\n","Epoch 28/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4109 - loss: 2.2328 - val_accuracy: 0.5034 - val_loss: 1.9232\n","Epoch 29/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4172 - loss: 2.2131 - val_accuracy: 0.5039 - val_loss: 1.9123\n","Epoch 30/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4095 - loss: 2.2178 - val_accuracy: 0.5182 - val_loss: 1.9024\n","Epoch 31/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4254 - loss: 2.1876 - val_accuracy: 0.5150 - val_loss: 1.8963\n","Epoch 32/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4207 - loss: 2.1893 - val_accuracy: 0.5258 - val_loss: 1.8773\n","Epoch 33/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4249 - loss: 2.1520 - val_accuracy: 0.5229 - val_loss: 1.8665\n","Epoch 34/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4264 - loss: 2.1588 - val_accuracy: 0.5279 - val_loss: 1.8457\n","Epoch 35/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4338 - loss: 2.1214 - val_accuracy: 0.5350 - val_loss: 1.8377\n","Epoch 36/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4357 - loss: 2.1436 - val_accuracy: 0.5358 - val_loss: 1.8253\n","Epoch 37/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4333 - loss: 2.1357 - val_accuracy: 0.5311 - val_loss: 1.8318\n","Epoch 38/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4374 - loss: 2.1159 - val_accuracy: 0.5371 - val_loss: 1.8101\n","Epoch 39/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4295 - loss: 2.1390 - val_accuracy: 0.5421 - val_loss: 1.7976\n","Epoch 40/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4469 - loss: 2.0983 - val_accuracy: 0.5429 - val_loss: 1.7844\n","Epoch 41/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4382 - loss: 2.0954 - val_accuracy: 0.5387 - val_loss: 1.7891\n","Epoch 42/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4444 - loss: 2.0796 - val_accuracy: 0.5487 - val_loss: 1.7693\n","Epoch 43/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4405 - loss: 2.0815 - val_accuracy: 0.5479 - val_loss: 1.7633\n","Epoch 44/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 2.1223 - val_accuracy: 0.5461 - val_loss: 1.7527\n","Epoch 45/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4433 - loss: 2.0766 - val_accuracy: 0.5497 - val_loss: 1.7582\n","Epoch 46/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4557 - loss: 2.0549 - val_accuracy: 0.5529 - val_loss: 1.7370\n","Epoch 47/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4490 - loss: 2.0351 - val_accuracy: 0.5611 - val_loss: 1.7328\n","Epoch 48/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4521 - loss: 2.0401 - val_accuracy: 0.5611 - val_loss: 1.7215\n","Epoch 49/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4574 - loss: 2.0383 - val_accuracy: 0.5597 - val_loss: 1.7200\n","Epoch 50/50\n","\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4522 - loss: 2.0487 - val_accuracy: 0.5626 - val_loss: 1.7059\n","\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5584 - loss: 1.7233\n","\n","Test Accuracy: 0.5626\n","\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n","F1 Score (Weighted): 0.5496\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.19      0.31        42\n","           1       0.77      0.64      0.70        42\n","           2       0.46      0.74      0.57        35\n","           3       0.43      0.34      0.38        29\n","           4       0.59      0.44      0.50        39\n","           5       0.68      0.38      0.49        45\n","           6       0.50      0.77      0.61        44\n","           7       0.29      0.36      0.32        36\n","           8       0.46      0.18      0.26        34\n","           9       0.34      0.45      0.39        31\n","          10       0.64      0.53      0.58        47\n","          11       0.71      0.57      0.63        35\n","          12       0.53      0.45      0.49        42\n","          13       0.86      0.15      0.26        40\n","          14       0.60      0.33      0.43        36\n","          15       0.28      0.31      0.30        35\n","          16       0.88      0.53      0.67        43\n","          17       0.45      0.36      0.40        47\n","          18       0.82      0.73      0.77        37\n","          19       0.52      0.65      0.58        37\n","          20       0.89      0.91      0.90        44\n","          21       0.51      0.61      0.56        41\n","          22       0.44      0.47      0.46        40\n","          23       0.63      0.49      0.55        35\n","          24       0.28      0.20      0.23        46\n","          25       0.40      0.06      0.10        36\n","          26       0.44      0.49      0.46        37\n","          27       0.76      0.64      0.70        45\n","          28       0.77      0.47      0.59        36\n","          29       0.48      0.64      0.55        47\n","          30       0.60      0.72      0.65        46\n","          31       0.36      0.49      0.41        37\n","          32       0.68      0.39      0.50        38\n","          33       0.58      0.92      0.71        37\n","          34       0.54      0.32      0.40        41\n","          35       0.79      0.87      0.82        38\n","          36       0.62      0.74      0.68        35\n","          37       0.67      0.23      0.34        53\n","          38       0.39      0.60      0.47        53\n","          39       0.66      0.72      0.69        46\n","          40       0.52      0.53      0.52        51\n","          41       0.65      0.81      0.72        43\n","          42       0.52      0.72      0.60        39\n","          43       0.87      0.66      0.75        41\n","          44       0.51      0.95      0.67        44\n","          45       0.36      0.40      0.38        35\n","          46       0.60      0.43      0.50        35\n","          47       0.52      0.36      0.42        39\n","          48       0.45      0.68      0.54        37\n","          49       0.71      0.28      0.40        43\n","          50       0.79      0.69      0.73        32\n","          51       0.38      0.63      0.48        38\n","          52       0.83      0.41      0.55        49\n","          53       0.44      0.55      0.49        31\n","          54       0.70      0.45      0.55        47\n","          55       0.52      0.27      0.35        41\n","          56       0.74      0.89      0.81        47\n","          57       0.67      0.67      0.67        36\n","          58       0.41      0.83      0.55        35\n","          59       0.60      0.82      0.70        39\n","          60       0.72      0.72      0.72        40\n","          61       0.59      0.29      0.39        56\n","          62       0.76      0.44      0.56        57\n","          63       0.39      0.56      0.46        27\n","          64       0.65      0.62      0.63        39\n","          65       0.46      0.49      0.47        35\n","          66       0.72      0.67      0.70        46\n","          67       0.77      0.54      0.63        37\n","          68       0.79      0.54      0.64        48\n","          69       0.47      0.65      0.54        43\n","          70       0.47      0.85      0.60        34\n","          71       0.38      0.75      0.50        36\n","          72       0.79      0.49      0.60        39\n","          73       0.57      0.95      0.72        41\n","          74       0.52      0.45      0.48        53\n","          75       0.63      0.97      0.77        37\n","          76       0.78      0.89      0.83        44\n","          77       0.47      0.23      0.31        30\n","          78       0.59      0.64      0.61        36\n","          79       0.53      0.63      0.58        30\n","          80       0.61      0.82      0.70        40\n","          81       0.55      0.27      0.36        45\n","          82       0.68      0.38      0.49        45\n","          83       0.45      0.55      0.49        31\n","          84       0.53      0.72      0.61        40\n","          85       0.79      0.86      0.83        44\n","          86       0.53      0.84      0.65        37\n","          87       0.44      0.66      0.53        35\n","          88       0.69      0.65      0.67        37\n","          89       0.33      0.42      0.37        38\n","          90       0.51      0.75      0.61        36\n","          91       0.49      0.44      0.46        39\n","          92       0.53      0.26      0.35        35\n","          93       0.74      0.87      0.80        39\n","          94       0.52      0.62      0.57        42\n","\n","    accuracy                           0.56      3800\n","   macro avg       0.58      0.56      0.55      3800\n","weighted avg       0.59      0.56      0.55      3800\n","\n"]}]}]}